{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de906ceb",
   "metadata": {},
   "source": [
    "# REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd6e05",
   "metadata": {},
   "source": [
    "## Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8980d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV file with the correct separator\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/reviews_data/movies_reviews2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipinitialspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Clean up column names by removing any leading/trailing commas\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file with the correct separator\n",
    "df = pd.read_csv('data/reviews_data/movies_reviews2.csv', sep=',', skipinitialspace=True)\n",
    "\n",
    "# Clean up column names by removing any leading/trailing commas\n",
    "df.columns = df.columns.str.strip(',')\n",
    "\n",
    "# Clean movie titles by removing content within parentheses\n",
    "df['title'] = df['title'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "\n",
    "# Select only the desired columns\n",
    "filtered_df = df[[\n",
    "    'title',\n",
    "    'quote',\n",
    "    'author'\n",
    "]]\n",
    "\n",
    "# Save the filtered data to a new CSV\n",
    "filtered_df.to_csv('data/reviews_data/filtered_reviews2.csv', index=False)\n",
    "\n",
    "# Print the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows of filtered data:\")\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac482077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   1%|▏         | 1/67 [02:55<3:12:55, 175.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 10,000 / 667,536 reviews in 175.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  16%|█▋        | 11/67 [32:48<2:47:36, 179.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 110,000 / 667,536 reviews in 1968.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  31%|███▏      | 21/67 [1:02:33<2:16:42, 178.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 210,000 / 667,536 reviews in 3753.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  46%|████▋     | 31/67 [1:32:32<1:48:09, 180.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 310,000 / 667,536 reviews in 5552.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  61%|██████    | 41/67 [2:02:55<1:18:16, 180.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 410,000 / 667,536 reviews in 7375.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  76%|███████▌  | 51/67 [2:32:34<47:22, 177.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 510,000 / 667,536 reviews in 9154.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  91%|█████████ | 61/67 [3:01:58<17:33, 175.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 610,000 / 667,536 reviews in 10918.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 67/67 [3:18:53<00:00, 178.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed in 3.32 hours\n",
      "Number of reviews after fuzzy matching: 358,152\n",
      "Number of unique movies in filtered reviews: 5,532\n"
     ]
    }
   ],
   "source": [
    "# Read the TSV file\n",
    "movies_df = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "# Read the filtered reviews CSV\n",
    "reviews_df = pd.read_csv('data/reviews_data/filtered_reviews2.csv')\n",
    "\n",
    "# Function to find best matching movie title with error handling\n",
    "def find_best_match(title, movie_titles_dict, threshold=80):\n",
    "    try:\n",
    "        # Handle NaN or non-string values\n",
    "        if pd.isna(title) or not isinstance(title, str):\n",
    "            return None\n",
    "            \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for movie_title, movie_id in movie_titles_dict.items():\n",
    "            score = fuzz.ratio(title.lower(), movie_title.lower())\n",
    "            if score > threshold and score > best_score:\n",
    "                best_score = score\n",
    "                best_match = movie_id\n",
    "        \n",
    "        return best_match\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing title: {title}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create dictionary of movie titles to tconst\n",
    "title_to_tconst = dict(zip(movies_df['primaryTitle'], movies_df['tconst']))\n",
    "\n",
    "# Start time measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in chunks to allow saving progress\n",
    "chunk_size = 10000\n",
    "total_chunks = len(reviews_df) // chunk_size + 1\n",
    "\n",
    "for chunk_start in tqdm(range(0, len(reviews_df), chunk_size), desc=\"Processing chunks\"):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(reviews_df))\n",
    "    chunk = reviews_df.iloc[chunk_start:chunk_end].copy()\n",
    "    \n",
    "    # Process chunk\n",
    "    chunk['tconst'] = chunk['title'].apply(lambda x: find_best_match(x, title_to_tconst))\n",
    "    \n",
    "    # Save progress to temporary file\n",
    "    chunk.to_csv(f'data/temp_chunk_{chunk_start}.csv', index=False)\n",
    "    \n",
    "    # Print progress\n",
    "    if chunk_start % (chunk_size * 10) == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nProcessed {chunk_end:,} / {len(reviews_df):,} reviews in {elapsed:.2f} seconds\")\n",
    "\n",
    "# Combine all chunks\n",
    "all_chunks = []\n",
    "for chunk_start in range(0, len(reviews_df), chunk_size):\n",
    "    chunk = pd.read_csv(f'data/temp_chunk_{chunk_start}.csv')\n",
    "    all_chunks.append(chunk)\n",
    "\n",
    "reviews_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# Remove rows where no match was found\n",
    "reviews_df = reviews_df.dropna(subset=['tconst'])\n",
    "\n",
    "# Save the final filtered data\n",
    "reviews_df.to_csv('data/filtered_reviews2.csv', index=False)\n",
    "\n",
    "# Print final statistics\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nProcessing completed in {elapsed_time/3600:.2f} hours\")\n",
    "print(f\"Number of reviews after fuzzy matching: {len(reviews_df):,}\")\n",
    "print(f\"Number of unique movies in filtered reviews: {reviews_df['title'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa252499",
   "metadata": {},
   "source": [
    "## Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5535ec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Statistics:\n",
      "Total rows after cleaning: 52675\n",
      "Unique series titles: 2519\n",
      "Unique authors: 1500\n",
      "\n",
      "Sample of cleaned data:\n",
      "     title                                     review_content  \\\n",
      "0  Rectify  It allows us to know and care for these charac...   \n",
      "1  Rectify  Rectify, a drama entering its final season on ...   \n",
      "2  Rectify  No other series so poignantly probes the human...   \n",
      "3  Rectify  None of these characters is particularly happy...   \n",
      "4  Rectify  Rectify is the best series I have ever seen on...   \n",
      "\n",
      "              author  \n",
      "0      Allison Keene  \n",
      "1   James Poniewozik  \n",
      "2  Melanie Mcfarland  \n",
      "3         Ken Tucker  \n",
      "4      Malcolm Jones  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52675 entries, 0 to 52674\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           52675 non-null  object\n",
      " 1   review_content  52675 non-null  object\n",
      " 2   author          52675 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('data/reviews_data/series_reviews1.csv')\n",
    "\n",
    "# Keep only needed columns\n",
    "df = df[['title', 'review_content', 'author']]\n",
    "\n",
    "# Clean the title by removing season number and any text in parentheses \n",
    "def clean_title(title):\n",
    "    try:\n",
    "        if pd.isna(title) or not isinstance(title, str):\n",
    "            return None\n",
    "        # Remove season part\n",
    "        title = title.split(':')[0]\n",
    "        # Remove text in parentheses\n",
    "        title = title.split('(')[0]\n",
    "        # Remove extra whitespace and convert to title case\n",
    "        title = ' '.join(title.split()).strip().title()\n",
    "        return title if title else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Clean review content\n",
    "def clean_review(review):\n",
    "    try:\n",
    "        if pd.isna(review) or not isinstance(review, str):\n",
    "            return None\n",
    "        # Remove extra whitespace and newlines\n",
    "        review = ' '.join(review.split())\n",
    "        # Remove empty reviews\n",
    "        return review if len(review.strip()) > 0 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Clean author name\n",
    "def clean_author(author):\n",
    "    try:\n",
    "        if pd.isna(author) or not isinstance(author, str):\n",
    "            return None\n",
    "        # Remove extra whitespace and convert to title case\n",
    "        author = ' '.join(author.split()).strip().title()\n",
    "        return author if author else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply cleaning functions\n",
    "df['title'] = df['title'].apply(clean_title)\n",
    "df['review_content'] = df['review_content'].apply(clean_review)\n",
    "df['author'] = df['author'].apply(clean_author)\n",
    "\n",
    "# Remove rows with any null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset index after cleaning\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv('data/reviews_data/cleaned_series_reviews.csv', index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nCleaning Statistics:\")\n",
    "print(f\"Total rows after cleaning: {len(df)}\")\n",
    "print(f\"Unique series titles: {df['title'].nunique()}\")\n",
    "print(f\"Unique authors: {df['author'].nunique()}\")\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feea186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   2%|▏         | 1/53 [00:05<04:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1,000 / 52,675 reviews in 5.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   6%|▌         | 3/53 [00:14<03:52,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 3,000 / 52,675 reviews in 14.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   9%|▉         | 5/53 [00:22<03:35,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 5,000 / 52,675 reviews in 22.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  13%|█▎        | 7/53 [00:31<03:23,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 7,000 / 52,675 reviews in 31.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  17%|█▋        | 9/53 [00:40<03:15,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 9,000 / 52,675 reviews in 40.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  21%|██        | 11/53 [00:49<03:07,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 11,000 / 52,675 reviews in 49.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  25%|██▍       | 13/53 [00:58<02:56,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 13,000 / 52,675 reviews in 58.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  28%|██▊       | 15/53 [01:07<02:49,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 15,000 / 52,675 reviews in 67.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  32%|███▏      | 17/53 [01:16<02:39,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 17,000 / 52,675 reviews in 76.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  36%|███▌      | 19/53 [01:25<02:34,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 19,000 / 52,675 reviews in 85.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  40%|███▉      | 21/53 [01:34<02:27,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 21,000 / 52,675 reviews in 94.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  43%|████▎     | 23/53 [01:43<02:15,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 23,000 / 52,675 reviews in 103.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  47%|████▋     | 25/53 [01:52<02:05,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 25,000 / 52,675 reviews in 112.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  51%|█████     | 27/53 [02:01<01:56,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 27,000 / 52,675 reviews in 121.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  55%|█████▍    | 29/53 [02:10<01:47,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 29,000 / 52,675 reviews in 130.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  58%|█████▊    | 31/53 [02:19<01:37,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 31,000 / 52,675 reviews in 139.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  62%|██████▏   | 33/53 [02:27<01:27,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 33,000 / 52,675 reviews in 147.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  66%|██████▌   | 35/53 [02:36<01:19,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 35,000 / 52,675 reviews in 156.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  70%|██████▉   | 37/53 [02:45<01:11,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 37,000 / 52,675 reviews in 165.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  74%|███████▎  | 39/53 [02:54<01:02,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 39,000 / 52,675 reviews in 174.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  77%|███████▋  | 41/53 [03:04<00:56,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 41,000 / 52,675 reviews in 184.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  81%|████████  | 43/53 [03:14<00:47,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 43,000 / 52,675 reviews in 194.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  85%|████████▍ | 45/53 [03:23<00:36,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 45,000 / 52,675 reviews in 203.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  89%|████████▊ | 47/53 [03:31<00:27,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 47,000 / 52,675 reviews in 211.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  92%|█████████▏| 49/53 [03:41<00:18,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 49,000 / 52,675 reviews in 221.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  96%|█████████▌| 51/53 [03:50<00:08,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 51,000 / 52,675 reviews in 230.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 53/53 [03:57<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 52,675 / 52,675 reviews in 237.52 seconds\n",
      "\n",
      "Combining successful chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data integrity check:\n",
      "Original records: 52675\n",
      "Processed records: 52675\n",
      "\n",
      "Processing completed in 0.07 hours\n",
      "Number of reviews after fuzzy matching: 27,298\n",
      "Number of unique series in filtered reviews: 960\n",
      "Success rate: 51.82%\n",
      "\n",
      "Sample of matched reviews:\n",
      "     title primaryTitle     tconst\n",
      "0  Rectify      Rectify  tt2183404\n",
      "1  Rectify      Rectify  tt2183404\n",
      "2  Rectify      Rectify  tt2183404\n",
      "3  Rectify      Rectify  tt2183404\n",
      "4  Rectify      Rectify  tt2183404\n",
      "\n",
      "Cleaning up temporary files...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Read the datasets\n",
    "series_reviews = pd.read_csv('data/reviews_data/cleaned_series_reviews.csv')\n",
    "final_data = pd.read_csv('data/FinalData.tsv', sep='\\t')\n",
    "\n",
    "# Filter only TV series from final_data\n",
    "tv_series = final_data[final_data['titleType'] == 'tvSeries']\n",
    "\n",
    "def find_best_match(title, series_dict, threshold=80):\n",
    "    try:\n",
    "        if pd.isna(title) or not isinstance(title, str):\n",
    "            return None\n",
    "            \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for series_title, series_id in series_dict.items():\n",
    "            score = fuzz.ratio(title.lower(), series_title.lower())\n",
    "            if score > threshold and score > best_score:\n",
    "                best_score = score\n",
    "                best_match = series_id\n",
    "        \n",
    "        return best_match\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing title: {title}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create dictionary of series titles to tconst\n",
    "title_to_tconst = dict(zip(tv_series['primaryTitle'], tv_series['tconst']))\n",
    "\n",
    "# Start time measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 1000  # Reduced chunk size for better tracking\n",
    "total_chunks = len(series_reviews) // chunk_size + 1\n",
    "\n",
    "# Dictionary to store chunk processing status\n",
    "chunk_status = {}\n",
    "\n",
    "# First pass: Process chunks with verification\n",
    "for chunk_start in tqdm(range(0, len(series_reviews), chunk_size), desc=\"Processing chunks\"):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(series_reviews))\n",
    "    chunk = series_reviews.iloc[chunk_start:chunk_end].copy()\n",
    "    \n",
    "    try:\n",
    "        # Process chunk\n",
    "        chunk['tconst'] = chunk['title'].apply(lambda x: find_best_match(x, title_to_tconst))\n",
    "        \n",
    "        # Verify chunk has expected number of rows\n",
    "        if len(chunk) == (chunk_end - chunk_start):\n",
    "            chunk.to_csv(f'data/temp_series_chunk_{chunk_start}.csv', index=False)\n",
    "            chunk_status[chunk_start] = 'success'\n",
    "        else:\n",
    "            chunk_status[chunk_start] = 'size_mismatch'\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing chunk {chunk_start}-{chunk_end}: {str(e)}\")\n",
    "        chunk_status[chunk_start] = 'error'\n",
    "    \n",
    "    # Print progress\n",
    "    if chunk_start % (chunk_size * 2) == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nProcessed {chunk_end:,} / {len(series_reviews):,} reviews in {elapsed:.2f} seconds\")\n",
    "\n",
    "# Verify and reprocess failed chunks\n",
    "failed_chunks = [start for start, status in chunk_status.items() if status != 'success']\n",
    "if failed_chunks:\n",
    "    print(f\"\\nReprocessing {len(failed_chunks)} failed chunks...\")\n",
    "    for chunk_start in tqdm(failed_chunks):\n",
    "        chunk_end = min(chunk_start + chunk_size, len(series_reviews))\n",
    "        chunk = series_reviews.iloc[chunk_start:chunk_end].copy()\n",
    "        \n",
    "        try:\n",
    "            chunk['tconst'] = chunk['title'].apply(lambda x: find_best_match(x, title_to_tconst))\n",
    "            chunk.to_csv(f'data/temp_series_chunk_{chunk_start}.csv', index=False)\n",
    "            chunk_status[chunk_start] = 'success'\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to reprocess chunk {chunk_start}-{chunk_end}: {str(e)}\")\n",
    "\n",
    "# Combine all successful chunks\n",
    "print(\"\\nCombining successful chunks...\")\n",
    "all_chunks = []\n",
    "for chunk_start in range(0, len(series_reviews), chunk_size):\n",
    "    if chunk_status.get(chunk_start) == 'success':\n",
    "        try:\n",
    "            chunk = pd.read_csv(f'data/temp_series_chunk_{chunk_start}.csv')\n",
    "            all_chunks.append(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading chunk {chunk_start}: {str(e)}\")\n",
    "\n",
    "# Combine and verify\n",
    "series_reviews = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# Verify data integrity\n",
    "original_count = len(series_reviews)\n",
    "processed_count = sum(len(chunk) for chunk in all_chunks)\n",
    "print(f\"\\nData integrity check:\")\n",
    "print(f\"Original records: {original_count}\")\n",
    "print(f\"Processed records: {processed_count}\")\n",
    "\n",
    "# Remove rows where no match was found\n",
    "series_reviews = series_reviews.dropna(subset=['tconst'])\n",
    "\n",
    "# Save the final filtered data\n",
    "series_reviews.to_csv('data/reviews_data/filtered_reviews1.csv', index=False)\n",
    "\n",
    "# Print final statistics\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nProcessing completed in {elapsed_time/3600:.2f} hours\")\n",
    "print(f\"Number of reviews after fuzzy matching: {len(series_reviews):,}\")\n",
    "print(f\"Number of unique series in filtered reviews: {series_reviews['title'].nunique():,}\")\n",
    "print(f\"Success rate: {(len(series_reviews)/original_count)*100:.2f}%\")\n",
    "\n",
    "# Display sample of matched reviews\n",
    "print(\"\\nSample of matched reviews:\")\n",
    "matched_reviews_df = series_reviews.merge(\n",
    "    tv_series[['tconst', 'primaryTitle']], \n",
    "    on='tconst', \n",
    "    how='left'\n",
    ")\n",
    "print(matched_reviews_df[['title', 'primaryTitle', 'tconst']].head())\n",
    "\n",
    "# Clean up temporary files\n",
    "print(\"\\nCleaning up temporary files...\")\n",
    "for chunk_start in range(0, len(series_reviews), chunk_size):\n",
    "    try:\n",
    "        os.remove(f'data/temp_series_chunk_{chunk_start}.csv')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6e72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Reviews Statistics:\n",
      "Total reviews: 385435\n",
      "Unique titles: 6355\n",
      "Unique authors: 75359\n",
      "Reviews with tconst: 385435\n",
      "\n",
      "Sample of combined reviews:\n",
      "         title                                     review_content  \\\n",
      "0  The Leopard  Superb direction.\\r\\nPainfully accurate for th...   \n",
      "1  The Leopard  \"The Leopard\" is the epic war drama that \"Gone...   \n",
      "2  The Leopard  The Leopard, or better \"Il Gattopardo\", is a m...   \n",
      "3  The Leopard  You see, people, not only the Leopard is a suc...   \n",
      "4  The Leopard  Excellent characterization. Passionate story. ...   \n",
      "\n",
      "            author     tconst  \n",
      "0           OldFrt  tt0057091  \n",
      "1    ryancarroll88  tt0057091  \n",
      "2            Lumax  tt0057091  \n",
      "3   EpicLadySponge  tt0057091  \n",
      "4  Serrao_Brochado  tt0057091  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read both review files\n",
    "movies_reviews = pd.read_csv('data/reviews_data/filtered_reviews2.csv')\n",
    "series_reviews = pd.read_csv('data/reviews_data/filtered_reviews1.csv')\n",
    "\n",
    "# Rename columns in movies_reviews to match series_reviews\n",
    "movies_reviews = movies_reviews.rename(columns={\n",
    "    'quote': 'review_content'\n",
    "})\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_reviews = pd.concat([\n",
    "    movies_reviews[['title', 'review_content', 'author', 'tconst']],\n",
    "    series_reviews[['title', 'review_content', 'author', 'tconst']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Remove any duplicates\n",
    "combined_reviews = combined_reviews.drop_duplicates()\n",
    "\n",
    "# Remove rows where tconst is null\n",
    "combined_reviews = combined_reviews.dropna(subset=['tconst'])\n",
    "\n",
    "# Save combined dataset\n",
    "combined_reviews.to_csv('data/FinalReviews.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nCombined Reviews Statistics:\")\n",
    "print(f\"Total reviews: {len(combined_reviews)}\")\n",
    "print(f\"Unique titles: {combined_reviews['title'].nunique()}\")\n",
    "print(f\"Unique authors: {combined_reviews['author'].nunique()}\")\n",
    "print(f\"Reviews with tconst: {combined_reviews['tconst'].notna().sum()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of combined reviews:\")\n",
    "print(combined_reviews.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
