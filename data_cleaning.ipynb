{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84456438",
   "metadata": {},
   "source": [
    "## Data Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720444f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b45d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = pd.read_csv('data/title.basics.tsv', sep='\\t')\n",
    "all_ratings = pd.read_csv('data/title.ratings.tsv', sep='\\t')\n",
    "\n",
    "# Filter titles to only include movies and TV series\n",
    "movies_and_series = all_titles[all_titles['titleType'].isin(['movie', 'tvSeries'])]\n",
    "\n",
    "# Convert columns to correct data types\n",
    "all_ratings['averageRating'] = pd.to_numeric(all_ratings['averageRating'], errors='coerce')\n",
    "all_ratings['numVotes'] = pd.to_numeric(all_ratings['numVotes'], errors='coerce')\n",
    "\n",
    "# Merge titles and ratings, keeping only movies and TV series\n",
    "merged_data = movies_and_series.merge(all_ratings, on='tconst', how='inner')\n",
    "\n",
    "# Filter movies/series with at least 1000 votes\n",
    "df_filtered = merged_data[merged_data['numVotes'] >= 1000]\n",
    "\n",
    "# Sort by rating (descending), then by numVotes (descending for tie-breaking)\n",
    "df_sorted = df_filtered.sort_values(by=['averageRating', 'numVotes'], ascending=[False, False])\n",
    "\n",
    "# Save to new TSV file\n",
    "df_sorted.to_csv('data/filtered_sorted_with_ratings.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4d72aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean rating: 6.39\n",
      "Minimum votes threshold (75th percentile): 10202\n",
      "\n",
      "Top 10 movies/series by weighted rating:\n",
      "                      primaryTitle  averageRating  numVotes  weightedRating\n",
      "175388                Breaking Bad            9.5   2405005        9.486874\n",
      "67193     The Shawshank Redemption            9.3   3104334        9.290476\n",
      "153336  Avatar: The Last Airbender            9.3    417720        9.230683\n",
      "129412                    The Wire            9.3    413512        9.229994\n",
      "176712             Game of Thrones            9.2   2485498        9.188523\n",
      "39197                The Godfather            9.2   2163561        9.186824\n",
      "80066                 The Sopranos            9.2    545115        9.148422\n",
      "162488             The Dark Knight            9.1   3079402        9.091060\n",
      "308327             Attack on Titan            9.1    649760        9.058146\n",
      "234194                   Aspirants            9.1    316623        9.015484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_10268\\175845366.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['weightedRating'] = df_filtered.apply(calculate_weighted_rating, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# IMDb-style weighted rating system for top 10,000 movies and series\n",
    "\n",
    "# Calculate overall statistics for the weighted rating formula\n",
    "overall_mean_rating = df_filtered['averageRating'].mean()\n",
    "min_votes_required = df_filtered['numVotes'].quantile(0.75)  # Use 75th percentile as minimum\n",
    "\n",
    "print(f\"Overall mean rating: {overall_mean_rating:.2f}\")\n",
    "print(f\"Minimum votes threshold (75th percentile): {min_votes_required:.0f}\")\n",
    "\n",
    "# Apply Bayesian weighted rating formula\n",
    "# Weighted Rating = (v / (v + m)) * R + (m / (v + m)) * C\n",
    "# Where: v = votes, m = min votes, R = average rating, C = overall mean\n",
    "def calculate_weighted_rating(row):\n",
    "    v = row['numVotes']\n",
    "    R = row['averageRating']\n",
    "    m = min_votes_required\n",
    "    C = overall_mean_rating\n",
    "    \n",
    "    weighted_rating = (v / (v + m)) * R + (m / (v + m)) * C\n",
    "    return weighted_rating\n",
    "\n",
    "# Add weighted rating column\n",
    "df_filtered['weightedRating'] = df_filtered.apply(calculate_weighted_rating, axis=1)\n",
    "\n",
    "# Sort by weighted rating (descending), then by numVotes (descending for tie-breaking)\n",
    "df_weighted_sorted = df_filtered.sort_values(by=['weightedRating', 'numVotes'], ascending=[False, False])\n",
    "\n",
    "# Get top 10,000 based on weighted ratings\n",
    "top_10000_weighted = df_weighted_sorted.head(10000)\n",
    "\n",
    "# Save the weighted top 10,000 to file\n",
    "top_10000_weighted.to_csv('data/top_10000_weighted_ratings.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"\\nTop 10 movies/series by weighted rating:\")\n",
    "print(top_10000_weighted[['primaryTitle', 'averageRating', 'numVotes', 'weightedRating']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76639737",
   "metadata": {},
   "source": [
    "## Adding Cast to data we already have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e22374",
   "metadata": {},
   "source": [
    "### Clean title.principals.tsv to only have movies/series in top 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d23cbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean title.principals.tsv to only have movies/series in top 10000\n",
    "all_principals = pd.read_csv('data/title.principals.tsv', sep='\\t')\n",
    "top_10000_df = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "# Filter all_principals to only include titles in top_10000_df\n",
    "filtered_principals = all_principals[all_principals['tconst'].isin(top_10000_df['tconst'])]\n",
    "filtered_principals.to_csv('data/filtered_title_principals_top_10000.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5046652",
   "metadata": {},
   "source": [
    "### Getting top 3 actor/actress names and their respective characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc49e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting top 3 unique cast members for each title (top 10 sample)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  20%|██        | 2/10 [00:01<00:07,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0903747: Found 3 unique cast members.\n",
      "[('Bryan Cranston', 'Walter White'), ('Aaron Paul', 'Jesse Pinkman'), ('Anna Gunn', 'Skyler White')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  30%|███       | 3/10 [00:03<00:08,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0111161: Found 3 unique cast members.\n",
      "[('Tim Robbins', 'Andy Dufresne'), ('Morgan Freeman', \"Ellis Boyd 'Red' Redding\"), ('Bob Gunton', 'Warden Norton')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  40%|████      | 4/10 [00:05<00:08,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0417299: Found 3 unique cast members.\n",
      "[('Dee Bradley Baker', 'Appa'), ('Zach Tyler Eisen', 'Aang'), ('Mae Whitman', 'Katara')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  50%|█████     | 5/10 [00:06<00:07,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0306414: Found 3 unique cast members.\n",
      "[('Dominic West', \"Detective James 'Jimmy' McNulty\"), ('Lance Reddick', 'Lieutenant Cedric Daniels'), ('Sonja Sohn', \"Detective Shakima 'Kima' Greggs\")]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  60%|██████    | 6/10 [00:08<00:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0944947: Found 3 unique cast members.\n",
      "[('Emilia Clarke', 'Daenerys Targaryen'), ('Peter Dinklage', 'Tyrion Lannister'), ('Kit Harington', 'Jon Snow')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  70%|███████   | 7/10 [00:10<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0068646: Found 3 unique cast members.\n",
      "[('Marlon Brando', 'Don Vito Corleone'), ('Al Pacino', 'Michael'), ('James Caan', 'Sonny')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  80%|████████  | 8/10 [00:12<00:03,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0141842: Found 3 unique cast members.\n",
      "[('James Gandolfini', 'Tony Soprano'), ('Lorraine Bracco', 'Dr. Jennifer Melfi'), ('Edie Falco', 'Carmela Soprano')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data:  90%|█████████ | 9/10 [00:13<00:01,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt0468569: Found 3 unique cast members.\n",
      "[('Christian Bale', 'Bruce Wayne'), ('Heath Ledger', 'Joker'), ('Aaron Eckhart', 'Harvey Dent')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data: 100%|██████████| 10/10 [00:15<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt2560140: Found 3 unique cast members.\n",
      "[('Jessie James Grelle', 'Armin Arlert'), ('Bryce Papenbrook', 'Eren Jaeger'), ('Trina Nishimura', 'Mikasa Ackermann')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cast data: 100%|██████████| 10/10 [00:17<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tt14392248: Found 3 unique cast members.\n",
      "[('Naveen Kasturia', 'Abhilash Sharma'), ('Shivankit Singh Parihar', 'Guri'), ('Abhilash Thapliyal', 'SK')]\n",
      "\n",
      "Processing completed! Results saved to 'data/cast_data/top_10_with_cast.tsv'\n",
      "\n",
      "Sample results:\n",
      "                 primaryTitle  \\\n",
      "0                Breaking Bad   \n",
      "1    The Shawshank Redemption   \n",
      "2  Avatar: The Last Airbender   \n",
      "3                    The Wire   \n",
      "4             Game of Thrones   \n",
      "5               The Godfather   \n",
      "6                The Sopranos   \n",
      "7             The Dark Knight   \n",
      "8             Attack on Titan   \n",
      "9                   Aspirants   \n",
      "\n",
      "                                          top_3_cast  \n",
      "0  [(Bryan Cranston, Walter White), (Aaron Paul, ...  \n",
      "1  [(Tim Robbins, Andy Dufresne), (Morgan Freeman...  \n",
      "2  [(Dee Bradley Baker, Appa), (Zach Tyler Eisen,...  \n",
      "3  [(Dominic West, Detective James 'Jimmy' McNult...  \n",
      "4  [(Emilia Clarke, Daenerys Targaryen), (Peter D...  \n",
      "5  [(Marlon Brando, Don Vito Corleone), (Al Pacin...  \n",
      "6  [(James Gandolfini, Tony Soprano), (Lorraine B...  \n",
      "7  [(Christian Bale, Bruce Wayne), (Heath Ledger,...  \n",
      "8  [(Jessie James Grelle, Armin Arlert), (Bryce P...  \n",
      "9  [(Naveen Kasturia, Abhilash Sharma), (Shivanki...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_45972\\3485801738.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top10_sample['top_3_cast'] = top10_sample['tconst'].head(10).progress_apply(get_top_3_cast) # Remove .head(10) to process all 10,000 entries\n"
     ]
    }
   ],
   "source": [
    "principals = pd.read_csv('data/cast_data/filtered_title_principals_top_10000.tsv', sep='\\t')\n",
    "names = pd.read_csv('data/cast_data/name.basics.tsv', sep='\\t')\n",
    "top10000 = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "# Create a sample of top 10 entries for testing\n",
    "top10_sample = top10000.head(10)\n",
    "\n",
    "# Get each tconst from top10_sample and find the top 3 UNIQUE actors/actresses nconst in principals and their respective characters\n",
    "def get_top_3_cast(tconst):\n",
    "    cast = principals[principals['tconst'] == tconst]\n",
    "    cast = cast[cast['category'].isin(['actor', 'actress'])]\n",
    "    \n",
    "    result = []\n",
    "    seen_nconst = set()  # Track unique nconst values\n",
    "    \n",
    "    for _, row in cast.iterrows():\n",
    "        nconst = row['nconst']\n",
    "        \n",
    "        # Skip if we've already seen this nconst\n",
    "        if nconst in seen_nconst:\n",
    "            continue\n",
    "            \n",
    "        character = row['characters']\n",
    "        \n",
    "        # Clean the character field - remove brackets and extra quotes\n",
    "        if pd.notna(character) and character != '\\\\N':\n",
    "            import json\n",
    "            try:\n",
    "                # Try to parse as JSON first\n",
    "                character_list = json.loads(character)\n",
    "                if isinstance(character_list, list) and len(character_list) > 0:\n",
    "                    clean_character = character_list[0]  # Get first character name\n",
    "                else:\n",
    "                    clean_character = str(character)\n",
    "            except (json.JSONDecodeError, ValueError):\n",
    "                # If JSON parsing fails, try manual cleaning\n",
    "                clean_character = character.strip('[]\"').replace('\"\"', '\"')\n",
    "        else:\n",
    "            clean_character = \"Unknown Character\"\n",
    "        \n",
    "        name_row = names[names['nconst'] == nconst]\n",
    "        \n",
    "        if not name_row.empty:\n",
    "            actor_name = name_row.iloc[0]['primaryName']\n",
    "            result.append((actor_name, clean_character))\n",
    "            seen_nconst.add(nconst)\n",
    "            \n",
    "            # Stop when we have 3 unique cast members\n",
    "            if len(result) >= 3:\n",
    "                break\n",
    "    \n",
    "    print(f\"Processed {tconst}: Found {len(result)} unique cast members.\")\n",
    "    print(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply function with progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Processing cast data\")\n",
    "\n",
    "print(\"Extracting top 3 unique cast members for each title (top 10 sample)...\")\n",
    "top10_sample['top_3_cast'] = top10_sample['tconst'].head(10).progress_apply(get_top_3_cast) # Remove .head(10) to process all 10,000 entries\n",
    "\n",
    "# Save results\n",
    "top10_sample.to_csv('data/cast_data/top_10_with_cast.tsv', sep='\\t', index=False)\n",
    "print(f\"\\nProcessing completed! Results saved to 'data/cast_data/top_10_with_cast.tsv'\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "print(top10_sample[['primaryTitle', 'top_3_cast']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35eb5f",
   "metadata": {},
   "source": [
    "## Web Scraping for Movie/Series Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29b4f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   ---------------------------------------- 2/2 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.2 soupsieve-2.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting web scraping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping movies: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed! Found data for 10 entries\n",
      "Success rate: 10 / 10\n",
      "\n",
      "Sample scraped data:\n",
      "                        title  \\\n",
      "0                Breaking Bad   \n",
      "1    The Shawshank Redemption   \n",
      "2  Avatar: The Last Airbender   \n",
      "3                    The Wire   \n",
      "4             Game of Thrones   \n",
      "\n",
      "                                                 url scrape_status  \\\n",
      "0    https://www.streamwithvpn.com/breaking-bad-2008       success   \n",
      "1  https://www.streamwithvpn.com/the-shawshank-re...       success   \n",
      "2  https://www.streamwithvpn.com/avatar-the-last-...       success   \n",
      "3        https://www.streamwithvpn.com/the-wire-2002       success   \n",
      "4  https://www.streamwithvpn.com/game-of-thrones-...       success   \n",
      "\n",
      "                                         description  cast  \n",
      "0  Walter White, a New Mexico chemistry teacher, ...  None  \n",
      "1  Imprisoned in the 1940s for the double murder ...  None  \n",
      "2  In a war-torn world of elemental magic, a youn...  None  \n",
      "3  Told from the points of view of both the Balti...  None  \n",
      "4  Seven noble families fight for control of the ...  None  \n",
      "\n",
      "Sample data saved to 'data/top10000_final.tsv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Web scraping script for StreamWithVPN data\n",
    "%pip install beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the top 10,000 weighted ratings data\n",
    "top_10000_df = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "def clean_title_for_url(title):\n",
    "    \"\"\"\n",
    "    Clean and format movie/series title for URL generation\n",
    "    \"\"\"\n",
    "    # Remove special characters and replace spaces with hyphens\n",
    "    cleaned = re.sub(r'[^\\w\\s-]', '', title)\n",
    "    cleaned = re.sub(r'\\s+', '-', cleaned.strip())\n",
    "    return cleaned.lower()\n",
    "\n",
    "def generate_streamwithvpn_url(title, year):\n",
    "    \"\"\"\n",
    "    Generate StreamWithVPN URL based on title and year\n",
    "    Example: \"The Wolf's Call\" (2019) -> \"https://www.streamwithvpn.com/the-wolfs-call-2019\"\n",
    "    \"\"\"\n",
    "    clean_title = clean_title_for_url(title)\n",
    "    # Handle cases where year might be NaN or missing\n",
    "    if pd.isna(year):\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}\"\n",
    "    else:\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}-{int(year)}\"\n",
    "\n",
    "def scrape_movie_data(url, tconst, title, year, endYear, titleType, isAdult, runtime, genres, rating, numVotes):\n",
    "    \"\"\"\n",
    "    Scrape movie/series data from StreamWithVPN\n",
    "    Returns dictionary with description, cast, and streaming platforms\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add delay to be respectful to the server\n",
    "        time.sleep(1)\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Try the original URL first\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            # print(f\"✓ Success with original URL: {url}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code in [404, 403]:  # Page not found or forbidden\n",
    "                # Try without year\n",
    "                url_without_year = generate_streamwithvpn_url(title, None)\n",
    "                # print(f\"⚠ Original URL failed ({e.response.status_code}), trying without year: {url_without_year}\")\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url_without_year, headers=headers, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    # print(f\"✓ Success with URL without year: {url_without_year}\")\n",
    "                    # Update the URL in the data dictionary for accuracy\n",
    "                    url = url_without_year\n",
    "                except requests.exceptions.HTTPError:\n",
    "                    # print(f\"✗ Both URLs failed for {title}\")\n",
    "                    raise  # Re-raise the exception to be caught by outer try-catch\n",
    "            else:\n",
    "                raise  # Re-raise non-404/403 errors\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Initialize data dictionary\n",
    "        movie_data = {\n",
    "            'tconst': tconst,\n",
    "            'titleType': titleType,\n",
    "            'title': title,\n",
    "            'year': year,\n",
    "            'endYear': endYear,\n",
    "            'isAdult': isAdult,\n",
    "            'runtime': runtime,\n",
    "            'genres': genres,\n",
    "            'rating': rating,\n",
    "            'numVotes': numVotes,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'url': url,\n",
    "            'scrape_status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Extract DESCRIPTION - multiple approaches\n",
    "        description_element = soup.find('span', class_='rt-Text EntryDetailDescription_contentDescription__tXYGO EntryDetailDescription_expanded__3a0Gs')\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('EntryDetailDescription_contentDescription'))\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('contentDescription'))\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.select_one('span[class*=\"EntryDetailDescription_contentDescription\"]')\n",
    "        \n",
    "        if description_element:\n",
    "            movie_data['description'] = description_element.get_text(strip=True)\n",
    "            # print(f\"✓ Found description for {title}: {movie_data['description'][:100]}...\")\n",
    "        else:\n",
    "            # print(f\"✗ No description found for {title}\")\n",
    "            pass\n",
    "        \"\"\"\n",
    "        # Extract CAST information\n",
    "        cast_list = []\n",
    "        # Target container div\n",
    "        container_div = soup.find('div', class_='rt-Flex rt-r-fd-column rt-r-gap rt-r-px rt-r-pt rt-r-w', style='--gap: 2px; --pl: 16px; --pr: 16px; --pt: 8px; --width: 100%;')\n",
    "        if container_div:\n",
    "            cast_spans = container_div.find_all('span', {'data-accent-color': 'gray', 'class_': 'rt-Text rt-r-size-3 rt-r-weight-medium', 'style': 'min-width: 0px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;'})\n",
    "            if cast_spans:\n",
    "                for span in cast_spans:\n",
    "                    cast_list.append(span.get_text(strip=True))\n",
    "                movie_data['cast'] = ', '.join(cast_list)\n",
    "                print(f\"✓ Found cast for {title}: {movie_data['cast'][:100]}...\")\n",
    "            else:\n",
    "                print(f\"✗ Span not found for {title}\")\n",
    "        else:\n",
    "            print(f\"✗ Div not found for {title}\")\n",
    "\n",
    "        # Extract STREAMING PLATFORMS information\n",
    "        platform_elements = soup.find_all('h2', class_='rt-Heading rt-r-size-5 rt-r-weight-medium rt-r-ta-left')\n",
    "        if platform_elements:\n",
    "            platforms = [elem.get_text(strip=True) for elem in platform_elements]\n",
    "            movie_data['streaming_platforms'] = ', '.join(platforms)\n",
    "        else:\n",
    "            print(f\"✗ No streaming platforms found for {title}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        return movie_data\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error for {title}: {e}\")\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'request_error: {str(e)}'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error for {title}: {e}\")\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'parsing_error: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Initialize list to store scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Sample scraping for first 2 entries (for faster debugging)\n",
    "print(\"Starting web scraping\")\n",
    "sample_df = top_10000_df.head(10) # Change to 10000 for full run (2 for testing)\n",
    "\n",
    "for index, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Scraping movies\"):\n",
    "    tconst = row['tconst']\n",
    "    title = row['primaryTitle']\n",
    "    year = row['startYear']\n",
    "    endYear = row['endYear']\n",
    "    titleType = row['titleType']\n",
    "    isAdult = row['isAdult']\n",
    "    runtime = row['runtimeMinutes']\n",
    "    genres = row['genres']\n",
    "    rating = row['averageRating']\n",
    "    numVotes = row['numVotes']\n",
    "    \n",
    "    \n",
    "    # Generate URL\n",
    "    url = generate_streamwithvpn_url(title, year)\n",
    "    # print(f\"\\nScraping: {title} ({year}) - {url}\")\n",
    "    \n",
    "    # Scrape data\n",
    "    movie_data = scrape_movie_data(url, tconst, title, year, endYear, titleType, isAdult, runtime, genres, rating, numVotes)\n",
    "    scraped_data.append(movie_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "scraped_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nScraping completed! Found data for {len(scraped_df)} entries\")\n",
    "print(f\"Success rate: {len(scraped_df[scraped_df['scrape_status'] == 'success'])} / {len(scraped_df)}\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample scraped data:\")\n",
    "print(scraped_df[['title', 'url', 'scrape_status', 'description', 'cast']].head())\n",
    "\n",
    "# Save scraped data\n",
    "scraped_df.to_csv('data/top10000_final.tsv', sep='\\t', index=False)\n",
    "print(\"\\nSample data saved to 'data/top10000_final.tsv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a23ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.14.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   5%|▌         | 1/20 [17:18<5:28:53, 1038.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 0-500 completed\n",
      "Processed 500/10,000 movies in 0.29 hours\n",
      "Estimated time remaining: 5.48 hours\n",
      "Success rate in chunk: 447 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  10%|█         | 2/20 [34:34<5:11:01, 1036.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 500-1000 completed\n",
      "Processed 1,000/10,000 movies in 0.58 hours\n",
      "Estimated time remaining: 5.19 hours\n",
      "Success rate in chunk: 448 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  15%|█▌        | 3/20 [51:44<4:52:53, 1033.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1000-1500 completed\n",
      "Processed 1,500/10,000 movies in 0.86 hours\n",
      "Estimated time remaining: 4.89 hours\n",
      "Success rate in chunk: 438 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  20%|██        | 4/20 [1:08:45<4:34:22, 1028.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1500-2000 completed\n",
      "Processed 2,000/10,000 movies in 1.15 hours\n",
      "Estimated time remaining: 4.58 hours\n",
      "Success rate in chunk: 427 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  25%|██▌       | 5/20 [1:26:21<4:19:39, 1038.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 2000-2500 completed\n",
      "Processed 2,500/10,000 movies in 1.44 hours\n",
      "Estimated time remaining: 4.32 hours\n",
      "Success rate in chunk: 449 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  30%|███       | 6/20 [1:43:15<4:00:21, 1030.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 2500-3000 completed\n",
      "Processed 3,000/10,000 movies in 1.72 hours\n",
      "Estimated time remaining: 4.02 hours\n",
      "Success rate in chunk: 412 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  35%|███▌      | 7/20 [2:00:20<3:42:50, 1028.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 3000-3500 completed\n",
      "Processed 3,500/10,000 movies in 2.01 hours\n",
      "Estimated time remaining: 3.72 hours\n",
      "Success rate in chunk: 418 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  40%|████      | 8/20 [2:17:11<3:24:36, 1023.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 3500-4000 completed\n",
      "Processed 4,000/10,000 movies in 2.29 hours\n",
      "Estimated time remaining: 3.43 hours\n",
      "Success rate in chunk: 410 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  45%|████▌     | 9/20 [2:34:08<3:07:11, 1021.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 4000-4500 completed\n",
      "Processed 4,500/10,000 movies in 2.57 hours\n",
      "Estimated time remaining: 3.14 hours\n",
      "Success rate in chunk: 408 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  50%|█████     | 10/20 [2:50:59<2:49:39, 1017.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 4500-5000 completed\n",
      "Processed 5,000/10,000 movies in 2.85 hours\n",
      "Estimated time remaining: 2.85 hours\n",
      "Success rate in chunk: 407 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  55%|█████▌    | 11/20 [3:07:34<2:31:37, 1010.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 5000-5500 completed\n",
      "Processed 5,500/10,000 movies in 3.13 hours\n",
      "Estimated time remaining: 2.56 hours\n",
      "Success rate in chunk: 393 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  60%|██████    | 12/20 [3:24:20<2:14:36, 1009.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 5500-6000 completed\n",
      "Processed 6,000/10,000 movies in 3.41 hours\n",
      "Estimated time remaining: 2.27 hours\n",
      "Success rate in chunk: 390 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  65%|██████▌   | 13/20 [3:41:09<1:57:45, 1009.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 6000-6500 completed\n",
      "Processed 6,500/10,000 movies in 3.69 hours\n",
      "Estimated time remaining: 1.98 hours\n",
      "Success rate in chunk: 387 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  70%|███████   | 14/20 [3:57:39<1:40:19, 1003.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 6500-7000 completed\n",
      "Processed 7,000/10,000 movies in 3.96 hours\n",
      "Estimated time remaining: 1.70 hours\n",
      "Success rate in chunk: 378 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  75%|███████▌  | 15/20 [4:14:12<1:23:22, 1000.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 7000-7500 completed\n",
      "Processed 7,500/10,000 movies in 4.24 hours\n",
      "Estimated time remaining: 1.41 hours\n",
      "Success rate in chunk: 379 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  80%|████████  | 16/20 [4:30:48<1:06:35, 998.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 7500-8000 completed\n",
      "Processed 8,000/10,000 movies in 4.51 hours\n",
      "Estimated time remaining: 1.13 hours\n",
      "Success rate in chunk: 384 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  85%|████████▌ | 17/20 [4:47:15<49:45, 995.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 8000-8500 completed\n",
      "Processed 8,500/10,000 movies in 4.79 hours\n",
      "Estimated time remaining: 0.84 hours\n",
      "Success rate in chunk: 363 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  90%|█████████ | 18/20 [5:03:36<33:02, 991.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 8500-9000 completed\n",
      "Processed 9,000/10,000 movies in 5.06 hours\n",
      "Estimated time remaining: 0.56 hours\n",
      "Success rate in chunk: 346 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  95%|█████████▌| 19/20 [5:19:53<16:26, 986.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 9000-9500 completed\n",
      "Processed 9,500/10,000 movies in 5.33 hours\n",
      "Estimated time remaining: 0.28 hours\n",
      "Success rate in chunk: 352 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 20/20 [5:36:08<00:00, 1008.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 9500-10000 completed\n",
      "Processed 10,000/10,000 movies in 5.60 hours\n",
      "Estimated time remaining: 0.00 hours\n",
      "Success rate in chunk: 352 / 500\n",
      "\n",
      "Combining chunks...\n",
      "\n",
      "Processing completed in 5.60 hours\n",
      "Total movies processed: 10,000\n",
      "Overall success rate: 7,988 / 10,000\n"
     ]
    }
   ],
   "source": [
    "# Web scraping script for StreamWithVPN data\n",
    "%pip install beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the top 10,000 weighted ratings data\n",
    "top_10000_df = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "def clean_title_for_url(title):\n",
    "    \"\"\"\n",
    "    Clean and format movie/series title for URL generation\n",
    "    \"\"\"\n",
    "    # Remove special characters and replace spaces with hyphens\n",
    "    cleaned = re.sub(r'[^\\w\\s-]', '', title)\n",
    "    cleaned = re.sub(r'\\s+', '-', cleaned.strip())\n",
    "    return cleaned.lower()\n",
    "\n",
    "def generate_streamwithvpn_url(title, year):\n",
    "    \"\"\"\n",
    "    Generate StreamWithVPN URL based on title and year\n",
    "    Example: \"The Wolf's Call\" (2019) -> \"https://www.streamwithvpn.com/the-wolfs-call-2019\"\n",
    "    \"\"\"\n",
    "    clean_title = clean_title_for_url(title)\n",
    "    # Handle cases where year might be NaN or missing\n",
    "    if pd.isna(year):\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}\"\n",
    "    else:\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}-{int(year)}\"\n",
    "\n",
    "def scrape_movie_data(url, tconst, title, year, endYear, titleType, isAdult, runtime, genres, rating, numVotes):\n",
    "    \"\"\"\n",
    "    Scrape movie/series data from StreamWithVPN\n",
    "    Returns dictionary with description, cast, and streaming platforms\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add delay to be respectful to the server\n",
    "        time.sleep(1)\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code in [404, 403]:\n",
    "                url_without_year = generate_streamwithvpn_url(title, None)\n",
    "                try:\n",
    "                    response = requests.get(url_without_year, headers=headers, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    url = url_without_year\n",
    "                except requests.exceptions.HTTPError:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        movie_data = {\n",
    "            'tconst': tconst,\n",
    "            'titleType': titleType,\n",
    "            'title': title,\n",
    "            'year': year,\n",
    "            'endYear': endYear,\n",
    "            'isAdult': isAdult,\n",
    "            'runtime': runtime,\n",
    "            'genres': genres,\n",
    "            'rating': rating,\n",
    "            'numVotes': numVotes,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'url': url,\n",
    "            'scrape_status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Extract description\n",
    "        description_element = soup.find('span', class_='rt-Text EntryDetailDescription_contentDescription__tXYGO EntryDetailDescription_expanded__3a0Gs')\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('EntryDetailDescription_contentDescription'))\n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('contentDescription'))\n",
    "        if not description_element:\n",
    "            description_element = soup.select_one('span[class*=\"EntryDetailDescription_contentDescription\"]')\n",
    "        \n",
    "        if description_element:\n",
    "            movie_data['description'] = description_element.get_text(strip=True)\n",
    "        \n",
    "        return movie_data\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'request_error: {str(e)}'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'parsing_error: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Initialize variables\n",
    "chunk_size = 500  # Process 500 movies at a time\n",
    "total_chunks = len(top_10000_df) // chunk_size + 1\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in chunks\n",
    "for chunk_start in tqdm(range(0, len(top_10000_df), chunk_size), desc=\"Processing chunks\"):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(top_10000_df))\n",
    "    chunk_df = top_10000_df.iloc[chunk_start:chunk_end]\n",
    "    chunk_data = []\n",
    "    \n",
    "    # Process each movie in the chunk\n",
    "    for _, row in tqdm(chunk_df.iterrows(), total=len(chunk_df), desc=f\"Chunk {chunk_start//chunk_size + 1}/{total_chunks}\", leave=False):\n",
    "        try:\n",
    "            tconst = row['tconst']\n",
    "            title = row['primaryTitle']\n",
    "            year = row['startYear']\n",
    "            endYear = row['endYear']\n",
    "            titleType = row['titleType']\n",
    "            isAdult = row['isAdult']\n",
    "            runtime = row['runtimeMinutes']\n",
    "            genres = row['genres']\n",
    "            rating = row['averageRating']\n",
    "            numVotes = row['numVotes']\n",
    "            \n",
    "            url = generate_streamwithvpn_url(title, year)\n",
    "            movie_data = scrape_movie_data(url, tconst, title, year, endYear, \n",
    "                                         titleType, isAdult, runtime, genres, \n",
    "                                         rating, numVotes)\n",
    "            chunk_data.append(movie_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {title}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save chunk progress\n",
    "    chunk_df = pd.DataFrame(chunk_data)\n",
    "    chunk_df.to_csv(f'data/temp_scrape_chunk_{chunk_start}.csv', sep='\\t', index=False)\n",
    "    \n",
    "    # Print progress stats\n",
    "    elapsed = time.time() - start_time\n",
    "    processed = chunk_end\n",
    "    remaining = len(top_10000_df) - processed\n",
    "    rate = processed / elapsed\n",
    "    eta = remaining / rate if rate > 0 else 0\n",
    "    \n",
    "    print(f\"\\nChunk {chunk_start}-{chunk_end} completed\")\n",
    "    print(f\"Processed {processed:,}/{len(top_10000_df):,} movies in {elapsed/3600:.2f} hours\")\n",
    "    print(f\"Estimated time remaining: {eta/3600:.2f} hours\")\n",
    "    print(f\"Success rate in chunk: {len(chunk_df[chunk_df['scrape_status'] == 'success'])} / {len(chunk_df)}\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"\\nCombining chunks...\")\n",
    "all_chunks = []\n",
    "for chunk_start in range(0, len(top_10000_df), chunk_size):\n",
    "    try:\n",
    "        chunk = pd.read_csv(f'data/temp_scrape_chunk_{chunk_start}.csv', sep='\\t')\n",
    "        all_chunks.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading chunk {chunk_start}: {str(e)}\")\n",
    "\n",
    "scraped_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# Save final results\n",
    "scraped_df.to_csv('data/top10000_final.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Print final statistics\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nProcessing completed in {total_time/3600:.2f} hours\")\n",
    "print(f\"Total movies processed: {len(scraped_df):,}\")\n",
    "print(f\"Overall success rate: {len(scraped_df[scraped_df['scrape_status'] == 'success']):,} / {len(scraped_df):,}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "import os\n",
    "for chunk_start in range(0, len(top_10000_df), chunk_size):\n",
    "    try:\n",
    "        os.remove(f'data/temp_scrape_chunk_{chunk_start}.csv')\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
