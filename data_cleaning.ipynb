{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84456438",
   "metadata": {},
   "source": [
    "## Data Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720444f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b45d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = pd.read_csv('data/title.basics.tsv', sep='\\t')\n",
    "all_ratings = pd.read_csv('data/title.ratings.tsv', sep='\\t')\n",
    "\n",
    "# Filter titles to only include movies and TV series\n",
    "movies_and_series = all_titles[all_titles['titleType'].isin(['movie', 'tvSeries'])]\n",
    "\n",
    "# Convert columns to correct data types\n",
    "all_ratings['averageRating'] = pd.to_numeric(all_ratings['averageRating'], errors='coerce')\n",
    "all_ratings['numVotes'] = pd.to_numeric(all_ratings['numVotes'], errors='coerce')\n",
    "\n",
    "# Merge titles and ratings, keeping only movies and TV series\n",
    "merged_data = movies_and_series.merge(all_ratings, on='tconst', how='inner')\n",
    "\n",
    "# Filter movies/series with at least 1000 votes\n",
    "df_filtered = merged_data[merged_data['numVotes'] >= 1000]\n",
    "\n",
    "# Sort by rating (descending), then by numVotes (descending for tie-breaking)\n",
    "df_sorted = df_filtered.sort_values(by=['averageRating', 'numVotes'], ascending=[False, False])\n",
    "\n",
    "# Save to new TSV file\n",
    "df_sorted.to_csv('data/filtered_sorted_with_ratings.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4d72aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean rating: 6.39\n",
      "Minimum votes threshold (75th percentile): 10202\n",
      "\n",
      "Top 10 movies/series by weighted rating:\n",
      "                      primaryTitle  averageRating  numVotes  weightedRating\n",
      "175388                Breaking Bad            9.5   2405005        9.486874\n",
      "67193     The Shawshank Redemption            9.3   3104334        9.290476\n",
      "153336  Avatar: The Last Airbender            9.3    417720        9.230683\n",
      "129412                    The Wire            9.3    413512        9.229994\n",
      "176712             Game of Thrones            9.2   2485498        9.188523\n",
      "39197                The Godfather            9.2   2163561        9.186824\n",
      "80066                 The Sopranos            9.2    545115        9.148422\n",
      "162488             The Dark Knight            9.1   3079402        9.091060\n",
      "308327             Attack on Titan            9.1    649760        9.058146\n",
      "234194                   Aspirants            9.1    316623        9.015484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_10268\\175845366.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['weightedRating'] = df_filtered.apply(calculate_weighted_rating, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# IMDb-style weighted rating system for top 10,000 movies and series\n",
    "\n",
    "# Calculate overall statistics for the weighted rating formula\n",
    "overall_mean_rating = df_filtered['averageRating'].mean()\n",
    "min_votes_required = df_filtered['numVotes'].quantile(0.75)  # Use 75th percentile as minimum\n",
    "\n",
    "print(f\"Overall mean rating: {overall_mean_rating:.2f}\")\n",
    "print(f\"Minimum votes threshold (75th percentile): {min_votes_required:.0f}\")\n",
    "\n",
    "# Apply Bayesian weighted rating formula\n",
    "# Weighted Rating = (v / (v + m)) * R + (m / (v + m)) * C\n",
    "# Where: v = votes, m = min votes, R = average rating, C = overall mean\n",
    "def calculate_weighted_rating(row):\n",
    "    v = row['numVotes']\n",
    "    R = row['averageRating']\n",
    "    m = min_votes_required\n",
    "    C = overall_mean_rating\n",
    "    \n",
    "    weighted_rating = (v / (v + m)) * R + (m / (v + m)) * C\n",
    "    return weighted_rating\n",
    "\n",
    "# Add weighted rating column\n",
    "df_filtered['weightedRating'] = df_filtered.apply(calculate_weighted_rating, axis=1)\n",
    "\n",
    "# Sort by weighted rating (descending), then by numVotes (descending for tie-breaking)\n",
    "df_weighted_sorted = df_filtered.sort_values(by=['weightedRating', 'numVotes'], ascending=[False, False])\n",
    "\n",
    "# Get top 10,000 based on weighted ratings\n",
    "top_10000_weighted = df_weighted_sorted.head(10000)\n",
    "\n",
    "# Save the weighted top 10,000 to file\n",
    "top_10000_weighted.to_csv('data/top_10000_weighted_ratings.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"\\nTop 10 movies/series by weighted rating:\")\n",
    "print(top_10000_weighted[['primaryTitle', 'averageRating', 'numVotes', 'weightedRating']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76639737",
   "metadata": {},
   "source": [
    "## Adding Cast to data we already have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e22374",
   "metadata": {},
   "source": [
    "### Clean title.principals.tsv to only have movies/series in top 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d23cbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean title.principals.tsv to only have movies/series in top 10000\n",
    "all_principals = pd.read_csv('data/title.principals.tsv', sep='\\t')\n",
    "top_10000_df = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "# Filter all_principals to only include titles in top_10000_df\n",
    "filtered_principals = all_principals[all_principals['tconst'].isin(top_10000_df['tconst'])]\n",
    "filtered_principals.to_csv('data/filtered_title_principals_top_10000.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5046652",
   "metadata": {},
   "source": [
    "### Getting top 3 actor/actress names and their respective characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc49e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10000 entries in chunks of 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1: entries 1 to 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  10%|█         | 51/500 [01:25<12:46,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  20%|██        | 101/500 [02:48<11:20,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  30%|███       | 151/500 [04:12<09:53,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  40%|████      | 201/500 [05:44<08:54,  1.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  50%|█████     | 251/500 [07:10<07:12,  1.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  60%|██████    | 301/500 [08:33<05:48,  1.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  70%|███████   | 351/500 [10:02<04:40,  1.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1: 100%|██████████| 500/500 [14:12<00:00,  1.70s/it]\u001b[A\n",
      "Chunk 1: 100%|██████████| 500/500 [14:12<00:00,  1.70s/it]52.11s/it]\n",
      "Processing chunks:   5%|▌         | 1/20 [14:12<4:29:50, 852.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_001_00001_to_00500.tsv\n",
      "  Chunk 1 completed successfully!\n",
      "\n",
      "Processing chunk 2: entries 501 to 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2:  20%|██        | 101/500 [02:54<09:30,  1.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2:  30%|███       | 151/500 [04:15<10:07,  1.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2:  50%|█████     | 251/500 [07:05<06:15,  1.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2:  60%|██████    | 301/500 [08:26<04:37,  1.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2:  70%|███████   | 351/500 [09:49<03:21,  1.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2:  80%|████████  | 401/500 [11:10<02:12,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2: 100%|██████████| 500/500 [13:50<00:00,  1.66s/it]\u001b[A\n",
      "Processing chunks:  10%|█         | 2/20 [28:02<4:11:50, 839.49s/it]\n",
      "Processing chunks:  10%|█         | 2/20 [28:02<4:11:50, 839.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_002_00501_to_01000.tsv\n",
      "  Chunk 2 completed successfully!\n",
      "\n",
      "Processing chunk 3: entries 1001 to 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  10%|█         | 51/500 [01:24<12:46,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  20%|██        | 101/500 [02:50<11:16,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  30%|███       | 151/500 [04:15<08:07,  1.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  40%|████      | 201/500 [05:40<08:29,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  50%|█████     | 251/500 [07:04<07:04,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  60%|██████    | 301/500 [08:23<05:40,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  70%|███████   | 351/500 [09:44<04:12,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3:  80%|████████  | 402/500 [11:07<02:09,  1.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3: 100%|██████████| 500/500 [13:51<00:00,  1.66s/it]\u001b[A\n",
      "Chunk 3: 100%|██████████| 500/500 [13:51<00:00,  1.66s/it]35.81s/it]\n",
      "Processing chunks:  15%|█▌        | 3/20 [41:54<3:56:48, 835.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_003_01001_to_01500.tsv\n",
      "  Chunk 3 completed successfully!\n",
      "\n",
      "Processing chunk 4: entries 1501 to 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  10%|█         | 51/500 [01:25<12:45,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  20%|██        | 101/500 [02:49<11:12,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  30%|███       | 151/500 [04:10<08:10,  1.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  40%|████      | 201/500 [05:28<07:03,  1.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  50%|█████     | 251/500 [06:50<07:01,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  60%|██████    | 301/500 [08:12<05:36,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  70%|███████   | 351/500 [09:32<04:12,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4:  80%|████████  | 401/500 [10:54<02:49,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4: 100%|██████████| 500/500 [13:38<00:00,  1.64s/it]\u001b[A\n",
      "Processing chunks:  20%|██        | 4/20 [55:32<3:41:04, 829.01s/it]\n",
      "Processing chunks:  20%|██        | 4/20 [55:32<3:41:04, 829.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_004_01501_to_02000.tsv\n",
      "  Chunk 4 completed successfully!\n",
      "\n",
      "Processing chunk 5: entries 2001 to 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  10%|█         | 51/500 [01:24<12:45,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  20%|██        | 101/500 [02:44<10:58,  1.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  30%|███       | 151/500 [04:07<09:43,  1.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  40%|████      | 202/500 [05:24<06:27,  1.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  50%|█████     | 251/500 [06:45<06:10,  1.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  60%|██████    | 301/500 [08:09<05:37,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  70%|███████   | 351/500 [09:23<03:25,  1.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5:  80%|████████  | 401/500 [10:47<02:47,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 5: 100%|██████████| 500/500 [13:31<00:00,  1.62s/it]\u001b[A\n",
      "Chunk 5: 100%|██████████| 500/500 [13:31<00:00,  1.62s/it] 822.59s/it]\n",
      "Processing chunks:  25%|██▌       | 5/20 [1:09:03<3:25:38, 822.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_005_02001_to_02500.tsv\n",
      "  Chunk 5 completed successfully!\n",
      "\n",
      "Processing chunk 6: entries 2501 to 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  10%|█         | 51/500 [01:17<12:22,  1.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  20%|██        | 101/500 [02:40<08:39,  1.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  30%|███       | 151/500 [04:05<09:49,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  40%|████      | 201/500 [05:28<08:27,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  50%|█████     | 251/500 [06:49<07:01,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  60%|██████    | 301/500 [08:11<05:34,  1.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  70%|███████   | 351/500 [09:30<04:11,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6:  80%|████████  | 401/500 [10:52<02:46,  1.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 6: 100%|██████████| 500/500 [13:33<00:00,  1.63s/it]\u001b[A\n",
      "Chunk 6: 100%|██████████| 500/500 [13:33<00:00,  1.63s/it] 819.56s/it]\n",
      "Processing chunks:  30%|███       | 6/20 [1:22:37<3:11:13, 819.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_006_02501_to_03000.tsv\n",
      "  Chunk 6 completed successfully!\n",
      "\n",
      "Processing chunk 7: entries 3001 to 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  10%|█         | 51/500 [01:23<10:51,  1.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  20%|██        | 101/500 [02:43<10:28,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  30%|███       | 151/500 [04:01<09:52,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  40%|████      | 201/500 [05:22<08:25,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  50%|█████     | 251/500 [06:45<07:02,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  60%|██████    | 301/500 [08:06<05:35,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  70%|███████   | 351/500 [09:26<04:11,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7:  80%|████████  | 401/500 [10:50<02:47,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 7: 100%|██████████| 500/500 [13:28<00:00,  1.62s/it]\u001b[A\n",
      "Chunk 7: 100%|██████████| 500/500 [13:28<00:00,  1.62s/it] 815.99s/it]\n",
      "Processing chunks:  35%|███▌      | 7/20 [1:36:06<2:56:47, 815.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_007_03001_to_03500.tsv\n",
      "  Chunk 7 completed successfully!\n",
      "\n",
      "Processing chunk 8: entries 3501 to 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:  10%|█         | 51/500 [01:22<12:30,  1.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:  20%|██        | 101/500 [02:39<09:03,  1.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:  30%|███       | 151/500 [04:00<09:49,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:  50%|█████     | 251/500 [06:26<06:59,  1.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:  60%|██████    | 301/500 [07:41<05:20,  1.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:  70%|███████   | 351/500 [09:02<04:35,  1.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8:  80%|████████  | 401/500 [10:24<02:44,  1.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 8: 100%|██████████| 500/500 [13:09<00:00,  1.58s/it]\u001b[A\n",
      "Chunk 8: 100%|██████████| 500/500 [13:09<00:00,  1.58s/it] 807.41s/it]\n",
      "Processing chunks:  40%|████      | 8/20 [1:49:15<2:41:28, 807.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_008_03501_to_04000.tsv\n",
      "  Chunk 8 completed successfully!\n",
      "\n",
      "Processing chunk 9: entries 4001 to 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  10%|█         | 51/500 [01:20<12:39,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  20%|██        | 101/500 [02:42<11:18,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  30%|███       | 151/500 [04:05<09:52,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  40%|████      | 201/500 [05:28<08:03,  1.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  50%|█████     | 252/500 [06:41<05:09,  1.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  60%|██████    | 301/500 [07:52<06:19,  1.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  70%|███████   | 351/500 [09:15<04:14,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9:  80%|████████  | 401/500 [10:36<02:47,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 9: 100%|██████████| 500/500 [13:13<00:00,  1.59s/it]\u001b[A\n",
      "Chunk 9: 100%|██████████| 500/500 [13:13<00:00,  1.59s/it] 803.07s/it]\n",
      "Processing chunks:  45%|████▌     | 9/20 [2:02:28<2:27:13, 803.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_009_04001_to_04500.tsv\n",
      "  Chunk 9 completed successfully!\n",
      "\n",
      "Processing chunk 10: entries 4501 to 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  10%|█         | 51/500 [01:21<09:45,  1.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  20%|██        | 101/500 [02:42<11:02,  1.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  30%|███       | 151/500 [04:00<07:03,  1.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  40%|████      | 202/500 [05:18<06:16,  1.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  50%|█████     | 251/500 [06:34<05:30,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  60%|██████    | 301/500 [07:54<05:20,  1.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  70%|███████   | 352/500 [09:16<03:11,  1.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10:  80%|████████  | 401/500 [10:31<02:29,  1.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 10: 100%|██████████| 500/500 [13:12<00:00,  1.59s/it]\u001b[A\n",
      "Chunk 10: 100%|██████████| 500/500 [13:12<00:00,  1.59s/it] 799.93s/it]\n",
      "Processing chunks:  50%|█████     | 10/20 [2:15:41<2:13:19, 799.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_010_04501_to_05000.tsv\n",
      "  Chunk 10 completed successfully!\n",
      "\n",
      "Processing chunk 11: entries 5001 to 5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  10%|█         | 51/500 [01:18<12:42,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  20%|██        | 101/500 [02:33<09:21,  1.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  30%|███       | 151/500 [03:55<09:38,  1.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  40%|████      | 201/500 [05:08<08:21,  1.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  50%|█████     | 251/500 [06:22<05:42,  1.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  60%|██████    | 301/500 [07:42<05:28,  1.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  70%|███████   | 351/500 [09:01<04:11,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11:  80%|████████  | 401/500 [10:22<02:47,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 11: 100%|██████████| 500/500 [12:59<00:00,  1.56s/it]\u001b[A\n",
      "Chunk 11: 100%|██████████| 500/500 [12:59<00:00,  1.56s/it] 793.60s/it]\n",
      "Processing chunks:  55%|█████▌    | 11/20 [2:28:41<1:59:02, 793.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_011_05001_to_05500.tsv\n",
      "  Chunk 11 completed successfully!\n",
      "\n",
      "Processing chunk 12: entries 5501 to 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  10%|█         | 51/500 [01:23<12:34,  1.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  20%|██        | 102/500 [02:43<08:35,  1.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  30%|███       | 151/500 [04:02<09:51,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  40%|████      | 201/500 [05:23<08:29,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  50%|█████     | 251/500 [06:39<06:52,  1.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  60%|██████    | 301/500 [07:58<05:37,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  70%|███████   | 351/500 [09:15<04:08,  1.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12:  80%|████████  | 401/500 [10:32<02:19,  1.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 12: 100%|██████████| 500/500 [13:13<00:00,  1.59s/it]\u001b[A\n",
      "Chunk 12: 100%|██████████| 500/500 [13:13<00:00,  1.59s/it] 793.54s/it]\n",
      "Processing chunks:  60%|██████    | 12/20 [2:41:54<1:45:48, 793.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_012_05501_to_06000.tsv\n",
      "  Chunk 12 completed successfully!\n",
      "\n",
      "Processing chunk 13: entries 6001 to 6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  10%|█         | 51/500 [01:21<12:49,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  20%|██        | 101/500 [02:43<11:17,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  30%|███       | 151/500 [04:05<09:55,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  40%|████      | 202/500 [05:21<04:41,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  50%|█████     | 251/500 [06:42<07:04,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  60%|██████    | 301/500 [08:05<04:40,  1.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  70%|███████   | 351/500 [09:26<03:27,  1.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13:  80%|████████  | 401/500 [10:37<01:21,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 13: 100%|██████████| 500/500 [13:04<00:00,  1.57s/it]\u001b[A\n",
      "Chunk 13: 100%|██████████| 500/500 [13:04<00:00,  1.57s/it] 790.71s/it]\n",
      "Processing chunks:  65%|██████▌   | 13/20 [2:54:58<1:32:14, 790.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_013_06001_to_06500.tsv\n",
      "  Chunk 13 completed successfully!\n",
      "\n",
      "Processing chunk 14: entries 6501 to 7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  10%|█         | 51/500 [01:18<10:35,  1.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  20%|██        | 101/500 [02:35<11:16,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  30%|███       | 151/500 [03:57<09:59,  1.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  40%|████      | 201/500 [05:16<08:32,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  50%|█████     | 252/500 [06:38<05:26,  1.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  60%|██████    | 301/500 [07:54<05:27,  1.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  70%|███████   | 352/500 [09:16<02:54,  1.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14:  80%|████████  | 401/500 [10:35<02:32,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 14: 100%|██████████| 500/500 [13:02<00:00,  1.57s/it]\u001b[A\n",
      "Processing chunks:  70%|███████   | 14/20 [3:08:01<1:18:49, 788.30s/it]\n",
      "Processing chunks:  70%|███████   | 14/20 [3:08:01<1:18:49, 788.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_014_06501_to_07000.tsv\n",
      "  Chunk 14 completed successfully!\n",
      "\n",
      "Processing chunk 15: entries 7001 to 7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  10%|█         | 51/500 [01:16<09:36,  1.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  20%|██        | 101/500 [02:39<11:19,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  30%|███       | 151/500 [04:02<09:53,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  40%|████      | 201/500 [05:20<06:15,  1.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  50%|█████     | 251/500 [06:45<07:04,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  60%|██████    | 301/500 [08:05<05:06,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  70%|███████   | 351/500 [09:24<04:07,  1.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15:  80%|████████  | 401/500 [10:47<02:48,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 15: 100%|██████████| 500/500 [13:14<00:00,  1.59s/it]\u001b[A\n",
      "Chunk 15: 100%|██████████| 500/500 [13:14<00:00,  1.59s/it] 790.17s/it]\n",
      "Processing chunks:  75%|███████▌  | 15/20 [3:21:15<1:05:50, 790.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_015_07001_to_07500.tsv\n",
      "  Chunk 15 completed successfully!\n",
      "\n",
      "Processing chunk 16: entries 7501 to 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  10%|█         | 51/500 [01:21<11:08,  1.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  20%|██        | 101/500 [02:40<11:12,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  30%|███       | 151/500 [03:58<07:44,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  40%|████      | 201/500 [05:16<05:18,  1.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  50%|█████     | 251/500 [06:36<07:00,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  60%|██████    | 301/500 [07:50<03:50,  1.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  70%|███████   | 351/500 [09:13<04:15,  1.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16:  80%|████████  | 401/500 [10:33<02:13,  1.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 16: 100%|██████████| 500/500 [13:06<00:00,  1.57s/it]\u001b[A\n",
      "Processing chunks:  80%|████████  | 16/20 [3:34:22<52:35, 788.97s/it]  \n",
      "Processing chunks:  80%|████████  | 16/20 [3:34:22<52:35, 788.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_016_07501_to_08000.tsv\n",
      "  Chunk 16 completed successfully!\n",
      "\n",
      "Processing chunk 17: entries 8001 to 8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  10%|█         | 51/500 [01:17<12:28,  1.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  20%|██        | 101/500 [02:40<11:21,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  30%|███       | 151/500 [04:00<09:50,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  40%|████      | 201/500 [05:23<07:53,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  50%|█████     | 251/500 [06:39<07:01,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  60%|██████    | 302/500 [08:01<03:53,  1.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  70%|███████   | 351/500 [09:15<03:30,  1.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17:  80%|████████  | 401/500 [10:33<01:57,  1.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 17: 100%|██████████| 500/500 [13:09<00:00,  1.58s/it]\u001b[A\n",
      "Chunk 17: 100%|██████████| 500/500 [13:09<00:00,  1.58s/it]89.28s/it]\n",
      "Processing chunks:  85%|████████▌ | 17/20 [3:47:32<39:27, 789.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_017_08001_to_08500.tsv\n",
      "  Chunk 17 completed successfully!\n",
      "\n",
      "Processing chunk 18: entries 8501 to 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  10%|█         | 51/500 [01:41<12:48,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  20%|██        | 101/500 [03:02<10:16,  1.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  30%|███       | 151/500 [04:18<08:17,  1.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  40%|████      | 201/500 [05:37<08:31,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  50%|█████     | 251/500 [06:55<07:05,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  60%|██████    | 301/500 [08:11<05:41,  1.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  70%|███████   | 351/500 [09:32<03:50,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18:  80%|████████  | 401/500 [10:48<02:20,  1.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 18: 100%|██████████| 500/500 [13:19<00:00,  1.60s/it]\u001b[A\n",
      "Processing chunks:  90%|█████████ | 18/20 [4:00:51<26:24, 792.33s/it]\n",
      "Processing chunks:  90%|█████████ | 18/20 [4:00:51<26:24, 792.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_018_08501_to_09000.tsv\n",
      "  Chunk 18 completed successfully!\n",
      "\n",
      "Processing chunk 19: entries 9001 to 9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  10%|█         | 51/500 [01:20<11:11,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  20%|██        | 101/500 [02:39<08:49,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  30%|███       | 151/500 [03:53<07:31,  1.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  40%|████      | 201/500 [05:13<06:34,  1.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  51%|█████     | 253/500 [06:32<03:49,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  60%|██████    | 302/500 [07:48<03:55,  1.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  70%|███████   | 351/500 [09:05<04:18,  1.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19:  80%|████████  | 401/500 [10:24<02:38,  1.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 19: 100%|██████████| 500/500 [12:53<00:00,  1.55s/it]\u001b[A\n",
      "Chunk 19: 100%|██████████| 500/500 [12:53<00:00,  1.55s/it]86.81s/it]\n",
      "Processing chunks:  95%|█████████▌| 19/20 [4:13:45<13:06, 786.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_019_09001_to_09500.tsv\n",
      "  Chunk 19 completed successfully!\n",
      "\n",
      "Processing chunk 20: entries 9501 to 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:  10%|█         | 51/500 [01:25<12:28,  1.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:  20%|██        | 101/500 [02:41<08:45,  1.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 150/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:  30%|███       | 151/500 [03:57<09:24,  1.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:  40%|████      | 201/500 [05:18<08:27,  1.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 250/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:  50%|█████     | 251/500 [06:41<06:36,  1.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 350/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:  70%|███████   | 351/500 [09:12<04:04,  1.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20:  80%|████████  | 401/500 [10:25<02:47,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 450/500 in current chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 20: 100%|██████████| 500/500 [12:58<00:00,  1.56s/it]\u001b[A\n",
      "Chunk 20: 100%|██████████| 500/500 [12:58<00:00,  1.56s/it]00.19s/it]\n",
      "Processing chunks: 100%|██████████| 20/20 [4:26:43<00:00, 800.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/500 in current chunk\n",
      "  Saved chunk to: data/cast_data/chunks/chunk_020_09501_to_10000.tsv\n",
      "  Chunk 20 completed successfully!\n",
      "\n",
      "Combining all chunks...\n",
      "\n",
      "Processing completed! Final results saved to 'data/cast_data/top_10_with_cast.tsv'\n",
      "Total entries processed: 10000\n",
      "\n",
      "Sample results:\n",
      "                 primaryTitle  \\\n",
      "0                Breaking Bad   \n",
      "1    The Shawshank Redemption   \n",
      "2  Avatar: The Last Airbender   \n",
      "3                    The Wire   \n",
      "4             Game of Thrones   \n",
      "5               The Godfather   \n",
      "6                The Sopranos   \n",
      "7             The Dark Knight   \n",
      "8             Attack on Titan   \n",
      "9                   Aspirants   \n",
      "\n",
      "                                          top_3_cast  \n",
      "0  [(Bryan Cranston, Walter White), (Aaron Paul, ...  \n",
      "1  [(Tim Robbins, Andy Dufresne), (Morgan Freeman...  \n",
      "2  [(Dee Bradley Baker, Appa), (Zach Tyler Eisen,...  \n",
      "3  [(Dominic West, Detective James 'Jimmy' McNult...  \n",
      "4  [(Emilia Clarke, Daenerys Targaryen), (Peter D...  \n",
      "5  [(Marlon Brando, Don Vito Corleone), (Al Pacin...  \n",
      "6  [(James Gandolfini, Tony Soprano), (Lorraine B...  \n",
      "7  [(Christian Bale, Bruce Wayne), (Heath Ledger,...  \n",
      "8  [(Jessie James Grelle, Armin Arlert), (Bryce P...  \n",
      "9  [(Naveen Kasturia, Abhilash Sharma), (Shivanki...  \n",
      "\n",
      "Chunk files saved in 'data/cast_data/chunks' for backup\n",
      "You can delete the chunks folder once you've verified the final file is correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "principals = pd.read_csv('data/cast_data/filtered_title_principals_top_10000.tsv', sep='\\t')\n",
    "names = pd.read_csv('data/cast_data/name.basics.tsv', sep='\\t')\n",
    "top10000 = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "# Sample of top 10 entries for testing - change to top10000 for full processing\n",
    "top10_sample = top10000.head(10)  # Change to top10000 for full dataset\n",
    "\n",
    "# Get each tconst from dataset and find the top 3 UNIQUE actors/actresses nconst in principals and their respective characters\n",
    "def get_top_3_cast(tconst):\n",
    "    cast = principals[principals['tconst'] == tconst]\n",
    "    cast = cast[cast['category'].isin(['actor', 'actress'])]\n",
    "    \n",
    "    result = []\n",
    "    seen_nconst = set()  # Track unique nconst values\n",
    "    \n",
    "    for _, row in cast.iterrows():\n",
    "        nconst = row['nconst']\n",
    "        \n",
    "        # Skip if we've already seen this nconst\n",
    "        if nconst in seen_nconst:\n",
    "            continue\n",
    "            \n",
    "        character = row['characters']\n",
    "        \n",
    "        # Clean the character field - remove brackets and extra quotes\n",
    "        if pd.notna(character) and character != '\\\\N':\n",
    "            import json\n",
    "            try:\n",
    "                # Try to parse as JSON first\n",
    "                character_list = json.loads(character)\n",
    "                if isinstance(character_list, list) and len(character_list) > 0:\n",
    "                    clean_character = character_list[0]  # Get first character name\n",
    "                else:\n",
    "                    clean_character = str(character)\n",
    "            except (json.JSONDecodeError, ValueError):\n",
    "                # If JSON parsing fails, try manual cleaning\n",
    "                clean_character = character.strip('[]\"').replace('\"\"', '\"')\n",
    "        else:\n",
    "            clean_character = \"Unknown Character\"\n",
    "        \n",
    "        name_row = names[names['nconst'] == nconst]\n",
    "        \n",
    "        if not name_row.empty:\n",
    "            actor_name = name_row.iloc[0]['primaryName']\n",
    "            result.append((actor_name, clean_character))\n",
    "            seen_nconst.add(nconst)\n",
    "            \n",
    "            # Stop when we have 3 unique cast members\n",
    "            if len(result) >= 3:\n",
    "                break\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Chunk-based processing with automatic saving\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "chunk_size = 500\n",
    "dataset = top10000  # Change to top10000 for full processing\n",
    "output_dir = 'data/cast_data/chunks'\n",
    "final_output = 'data/cast_data/top_10_with_cast.tsv'  # Change to top_10000_with_cast.tsv for full processing\n",
    "\n",
    "# Create chunks directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing {len(dataset)} entries in chunks of {chunk_size}...\")\n",
    "\n",
    "# Process in chunks\n",
    "all_results = []\n",
    "for chunk_start in tqdm(range(0, len(dataset), chunk_size), desc=\"Processing chunks\"):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(dataset))\n",
    "    chunk_data = dataset.iloc[chunk_start:chunk_end].copy()\n",
    "    \n",
    "    print(f\"\\nProcessing chunk {chunk_start//chunk_size + 1}: entries {chunk_start+1} to {chunk_end}\")\n",
    "    \n",
    "    # Process each entry in the chunk\n",
    "    chunk_results = []\n",
    "    for idx, (_, row) in enumerate(tqdm(chunk_data.iterrows(), total=len(chunk_data), desc=f\"Chunk {chunk_start//chunk_size + 1}\")):\n",
    "        try:\n",
    "            cast_result = get_top_3_cast(row['tconst'])\n",
    "            chunk_results.append(cast_result)\n",
    "            \n",
    "            # Print progress every 50 entries within chunk\n",
    "            if (idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(chunk_data)} in current chunk\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {row['tconst']}: {e}\")\n",
    "            chunk_results.append([])  # Empty result for failed entries\n",
    "    \n",
    "    # Add results to chunk data\n",
    "    chunk_data['top_3_cast'] = chunk_results\n",
    "    \n",
    "    # Save chunk to file\n",
    "    chunk_filename = f\"{output_dir}/chunk_{chunk_start//chunk_size + 1:03d}_{chunk_start+1:05d}_to_{chunk_end:05d}.tsv\"\n",
    "    chunk_data.to_csv(chunk_filename, sep='\\t', index=False)\n",
    "    print(f\"  Saved chunk to: {chunk_filename}\")\n",
    "    \n",
    "    # Add to all results\n",
    "    all_results.append(chunk_data)\n",
    "    \n",
    "    print(f\"  Chunk {chunk_start//chunk_size + 1} completed successfully!\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"\\nCombining all chunks...\")\n",
    "final_dataset = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Save final combined result\n",
    "final_dataset.to_csv(final_output, sep='\\t', index=False)\n",
    "print(f\"\\nProcessing completed! Final results saved to '{final_output}'\")\n",
    "print(f\"Total entries processed: {len(final_dataset)}\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "print(final_dataset[['primaryTitle', 'top_3_cast']].head(10))\n",
    "\n",
    "# Clean up chunk files (optional - comment out if you want to keep them)\n",
    "print(f\"\\nChunk files saved in '{output_dir}' for backup\")\n",
    "print(\"You can delete the chunks folder once you've verified the final file is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35eb5f",
   "metadata": {},
   "source": [
    "## Web Scraping for Movie/Series Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b29b4f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   ---------------------------------------- 2/2 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.2 soupsieve-2.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting web scraping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping movies: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed! Found data for 10 entries\n",
      "Success rate: 10 / 10\n",
      "\n",
      "Sample scraped data:\n",
      "                        title  \\\n",
      "0                Breaking Bad   \n",
      "1    The Shawshank Redemption   \n",
      "2  Avatar: The Last Airbender   \n",
      "3                    The Wire   \n",
      "4             Game of Thrones   \n",
      "\n",
      "                                                 url scrape_status  \\\n",
      "0    https://www.streamwithvpn.com/breaking-bad-2008       success   \n",
      "1  https://www.streamwithvpn.com/the-shawshank-re...       success   \n",
      "2  https://www.streamwithvpn.com/avatar-the-last-...       success   \n",
      "3        https://www.streamwithvpn.com/the-wire-2002       success   \n",
      "4  https://www.streamwithvpn.com/game-of-thrones-...       success   \n",
      "\n",
      "                                         description  cast  \n",
      "0  Walter White, a New Mexico chemistry teacher, ...  None  \n",
      "1  Imprisoned in the 1940s for the double murder ...  None  \n",
      "2  In a war-torn world of elemental magic, a youn...  None  \n",
      "3  Told from the points of view of both the Balti...  None  \n",
      "4  Seven noble families fight for control of the ...  None  \n",
      "\n",
      "Sample data saved to 'data/top10000_final.tsv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Web scraping script for StreamWithVPN data\n",
    "%pip install beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the top 10,000 weighted ratings data\n",
    "top_10000_df = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "def clean_title_for_url(title):\n",
    "    \"\"\"\n",
    "    Clean and format movie/series title for URL generation\n",
    "    \"\"\"\n",
    "    # Remove special characters and replace spaces with hyphens\n",
    "    cleaned = re.sub(r'[^\\w\\s-]', '', title)\n",
    "    cleaned = re.sub(r'\\s+', '-', cleaned.strip())\n",
    "    return cleaned.lower()\n",
    "\n",
    "def generate_streamwithvpn_url(title, year):\n",
    "    \"\"\"\n",
    "    Generate StreamWithVPN URL based on title and year\n",
    "    Example: \"The Wolf's Call\" (2019) -> \"https://www.streamwithvpn.com/the-wolfs-call-2019\"\n",
    "    \"\"\"\n",
    "    clean_title = clean_title_for_url(title)\n",
    "    # Handle cases where year might be NaN or missing\n",
    "    if pd.isna(year):\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}\"\n",
    "    else:\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}-{int(year)}\"\n",
    "\n",
    "def scrape_movie_data(url, tconst, title, year, endYear, titleType, isAdult, runtime, genres, rating, numVotes):\n",
    "    \"\"\"\n",
    "    Scrape movie/series data from StreamWithVPN\n",
    "    Returns dictionary with description, cast, and streaming platforms\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add delay to be respectful to the server\n",
    "        time.sleep(1)\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Try the original URL first\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            # print(f\"✓ Success with original URL: {url}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code in [404, 403]:  # Page not found or forbidden\n",
    "                # Try without year\n",
    "                url_without_year = generate_streamwithvpn_url(title, None)\n",
    "                # print(f\"⚠ Original URL failed ({e.response.status_code}), trying without year: {url_without_year}\")\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url_without_year, headers=headers, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    # print(f\"✓ Success with URL without year: {url_without_year}\")\n",
    "                    # Update the URL in the data dictionary for accuracy\n",
    "                    url = url_without_year\n",
    "                except requests.exceptions.HTTPError:\n",
    "                    # print(f\"✗ Both URLs failed for {title}\")\n",
    "                    raise  # Re-raise the exception to be caught by outer try-catch\n",
    "            else:\n",
    "                raise  # Re-raise non-404/403 errors\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Initialize data dictionary\n",
    "        movie_data = {\n",
    "            'tconst': tconst,\n",
    "            'titleType': titleType,\n",
    "            'title': title,\n",
    "            'year': year,\n",
    "            'endYear': endYear,\n",
    "            'isAdult': isAdult,\n",
    "            'runtime': runtime,\n",
    "            'genres': genres,\n",
    "            'rating': rating,\n",
    "            'numVotes': numVotes,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'url': url,\n",
    "            'scrape_status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Extract DESCRIPTION - multiple approaches\n",
    "        description_element = soup.find('span', class_='rt-Text EntryDetailDescription_contentDescription__tXYGO EntryDetailDescription_expanded__3a0Gs')\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('EntryDetailDescription_contentDescription'))\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('contentDescription'))\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.select_one('span[class*=\"EntryDetailDescription_contentDescription\"]')\n",
    "        \n",
    "        if description_element:\n",
    "            movie_data['description'] = description_element.get_text(strip=True)\n",
    "            # print(f\"✓ Found description for {title}: {movie_data['description'][:100]}...\")\n",
    "        else:\n",
    "            # print(f\"✗ No description found for {title}\")\n",
    "            pass\n",
    "        \"\"\"\n",
    "        # Extract CAST information\n",
    "        cast_list = []\n",
    "        # Target container div\n",
    "        container_div = soup.find('div', class_='rt-Flex rt-r-fd-column rt-r-gap rt-r-px rt-r-pt rt-r-w', style='--gap: 2px; --pl: 16px; --pr: 16px; --pt: 8px; --width: 100%;')\n",
    "        if container_div:\n",
    "            cast_spans = container_div.find_all('span', {'data-accent-color': 'gray', 'class_': 'rt-Text rt-r-size-3 rt-r-weight-medium', 'style': 'min-width: 0px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;'})\n",
    "            if cast_spans:\n",
    "                for span in cast_spans:\n",
    "                    cast_list.append(span.get_text(strip=True))\n",
    "                movie_data['cast'] = ', '.join(cast_list)\n",
    "                print(f\"✓ Found cast for {title}: {movie_data['cast'][:100]}...\")\n",
    "            else:\n",
    "                print(f\"✗ Span not found for {title}\")\n",
    "        else:\n",
    "            print(f\"✗ Div not found for {title}\")\n",
    "\n",
    "        # Extract STREAMING PLATFORMS information\n",
    "        platform_elements = soup.find_all('h2', class_='rt-Heading rt-r-size-5 rt-r-weight-medium rt-r-ta-left')\n",
    "        if platform_elements:\n",
    "            platforms = [elem.get_text(strip=True) for elem in platform_elements]\n",
    "            movie_data['streaming_platforms'] = ', '.join(platforms)\n",
    "        else:\n",
    "            print(f\"✗ No streaming platforms found for {title}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        return movie_data\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error for {title}: {e}\")\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'request_error: {str(e)}'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error for {title}: {e}\")\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'parsing_error: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Initialize list to store scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Sample scraping for first 2 entries (for faster debugging)\n",
    "print(\"Starting web scraping\")\n",
    "sample_df = top_10000_df.head(10) # Change to 10000 for full run (2 for testing)\n",
    "\n",
    "for index, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Scraping movies\"):\n",
    "    tconst = row['tconst']\n",
    "    title = row['primaryTitle']\n",
    "    year = row['startYear']\n",
    "    endYear = row['endYear']\n",
    "    titleType = row['titleType']\n",
    "    isAdult = row['isAdult']\n",
    "    runtime = row['runtimeMinutes']\n",
    "    genres = row['genres']\n",
    "    rating = row['averageRating']\n",
    "    numVotes = row['numVotes']\n",
    "    \n",
    "    \n",
    "    # Generate URL\n",
    "    url = generate_streamwithvpn_url(title, year)\n",
    "    # print(f\"\\nScraping: {title} ({year}) - {url}\")\n",
    "    \n",
    "    # Scrape data\n",
    "    movie_data = scrape_movie_data(url, tconst, title, year, endYear, titleType, isAdult, runtime, genres, rating, numVotes)\n",
    "    scraped_data.append(movie_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "scraped_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nScraping completed! Found data for {len(scraped_df)} entries\")\n",
    "print(f\"Success rate: {len(scraped_df[scraped_df['scrape_status'] == 'success'])} / {len(scraped_df)}\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample scraped data:\")\n",
    "print(scraped_df[['title', 'url', 'scrape_status', 'description', 'cast']].head())\n",
    "\n",
    "# Save scraped data\n",
    "scraped_df.to_csv('data/top10000_final.tsv', sep='\\t', index=False)\n",
    "print(\"\\nSample data saved to 'data/top10000_final.tsv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a23ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.14.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\administrator\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   5%|▌         | 1/20 [17:18<5:28:53, 1038.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 0-500 completed\n",
      "Processed 500/10,000 movies in 0.29 hours\n",
      "Estimated time remaining: 5.48 hours\n",
      "Success rate in chunk: 447 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  10%|█         | 2/20 [34:34<5:11:01, 1036.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 500-1000 completed\n",
      "Processed 1,000/10,000 movies in 0.58 hours\n",
      "Estimated time remaining: 5.19 hours\n",
      "Success rate in chunk: 448 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  15%|█▌        | 3/20 [51:44<4:52:53, 1033.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1000-1500 completed\n",
      "Processed 1,500/10,000 movies in 0.86 hours\n",
      "Estimated time remaining: 4.89 hours\n",
      "Success rate in chunk: 438 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  20%|██        | 4/20 [1:08:45<4:34:22, 1028.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1500-2000 completed\n",
      "Processed 2,000/10,000 movies in 1.15 hours\n",
      "Estimated time remaining: 4.58 hours\n",
      "Success rate in chunk: 427 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  25%|██▌       | 5/20 [1:26:21<4:19:39, 1038.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 2000-2500 completed\n",
      "Processed 2,500/10,000 movies in 1.44 hours\n",
      "Estimated time remaining: 4.32 hours\n",
      "Success rate in chunk: 449 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  30%|███       | 6/20 [1:43:15<4:00:21, 1030.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 2500-3000 completed\n",
      "Processed 3,000/10,000 movies in 1.72 hours\n",
      "Estimated time remaining: 4.02 hours\n",
      "Success rate in chunk: 412 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  35%|███▌      | 7/20 [2:00:20<3:42:50, 1028.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 3000-3500 completed\n",
      "Processed 3,500/10,000 movies in 2.01 hours\n",
      "Estimated time remaining: 3.72 hours\n",
      "Success rate in chunk: 418 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  40%|████      | 8/20 [2:17:11<3:24:36, 1023.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 3500-4000 completed\n",
      "Processed 4,000/10,000 movies in 2.29 hours\n",
      "Estimated time remaining: 3.43 hours\n",
      "Success rate in chunk: 410 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  45%|████▌     | 9/20 [2:34:08<3:07:11, 1021.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 4000-4500 completed\n",
      "Processed 4,500/10,000 movies in 2.57 hours\n",
      "Estimated time remaining: 3.14 hours\n",
      "Success rate in chunk: 408 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  50%|█████     | 10/20 [2:50:59<2:49:39, 1017.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 4500-5000 completed\n",
      "Processed 5,000/10,000 movies in 2.85 hours\n",
      "Estimated time remaining: 2.85 hours\n",
      "Success rate in chunk: 407 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  55%|█████▌    | 11/20 [3:07:34<2:31:37, 1010.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 5000-5500 completed\n",
      "Processed 5,500/10,000 movies in 3.13 hours\n",
      "Estimated time remaining: 2.56 hours\n",
      "Success rate in chunk: 393 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  60%|██████    | 12/20 [3:24:20<2:14:36, 1009.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 5500-6000 completed\n",
      "Processed 6,000/10,000 movies in 3.41 hours\n",
      "Estimated time remaining: 2.27 hours\n",
      "Success rate in chunk: 390 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  65%|██████▌   | 13/20 [3:41:09<1:57:45, 1009.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 6000-6500 completed\n",
      "Processed 6,500/10,000 movies in 3.69 hours\n",
      "Estimated time remaining: 1.98 hours\n",
      "Success rate in chunk: 387 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  70%|███████   | 14/20 [3:57:39<1:40:19, 1003.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 6500-7000 completed\n",
      "Processed 7,000/10,000 movies in 3.96 hours\n",
      "Estimated time remaining: 1.70 hours\n",
      "Success rate in chunk: 378 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  75%|███████▌  | 15/20 [4:14:12<1:23:22, 1000.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 7000-7500 completed\n",
      "Processed 7,500/10,000 movies in 4.24 hours\n",
      "Estimated time remaining: 1.41 hours\n",
      "Success rate in chunk: 379 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  80%|████████  | 16/20 [4:30:48<1:06:35, 998.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 7500-8000 completed\n",
      "Processed 8,000/10,000 movies in 4.51 hours\n",
      "Estimated time remaining: 1.13 hours\n",
      "Success rate in chunk: 384 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  85%|████████▌ | 17/20 [4:47:15<49:45, 995.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 8000-8500 completed\n",
      "Processed 8,500/10,000 movies in 4.79 hours\n",
      "Estimated time remaining: 0.84 hours\n",
      "Success rate in chunk: 363 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  90%|█████████ | 18/20 [5:03:36<33:02, 991.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 8500-9000 completed\n",
      "Processed 9,000/10,000 movies in 5.06 hours\n",
      "Estimated time remaining: 0.56 hours\n",
      "Success rate in chunk: 346 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  95%|█████████▌| 19/20 [5:19:53<16:26, 986.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 9000-9500 completed\n",
      "Processed 9,500/10,000 movies in 5.33 hours\n",
      "Estimated time remaining: 0.28 hours\n",
      "Success rate in chunk: 352 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 20/20 [5:36:08<00:00, 1008.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 9500-10000 completed\n",
      "Processed 10,000/10,000 movies in 5.60 hours\n",
      "Estimated time remaining: 0.00 hours\n",
      "Success rate in chunk: 352 / 500\n",
      "\n",
      "Combining chunks...\n",
      "\n",
      "Processing completed in 5.60 hours\n",
      "Total movies processed: 10,000\n",
      "Overall success rate: 7,988 / 10,000\n"
     ]
    }
   ],
   "source": [
    "# Web scraping script for StreamWithVPN data\n",
    "%pip install beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the top 10,000 weighted ratings data\n",
    "top_10000_df = pd.read_csv('data/top_10000_weighted_ratings.tsv', sep='\\t')\n",
    "\n",
    "def clean_title_for_url(title):\n",
    "    \"\"\"\n",
    "    Clean and format movie/series title for URL generation\n",
    "    \"\"\"\n",
    "    # Remove special characters and replace spaces with hyphens\n",
    "    cleaned = re.sub(r'[^\\w\\s-]', '', title)\n",
    "    cleaned = re.sub(r'\\s+', '-', cleaned.strip())\n",
    "    return cleaned.lower()\n",
    "\n",
    "def generate_streamwithvpn_url(title, year):\n",
    "    \"\"\"\n",
    "    Generate StreamWithVPN URL based on title and year\n",
    "    Example: \"The Wolf's Call\" (2019) -> \"https://www.streamwithvpn.com/the-wolfs-call-2019\"\n",
    "    \"\"\"\n",
    "    clean_title = clean_title_for_url(title)\n",
    "    # Handle cases where year might be NaN or missing\n",
    "    if pd.isna(year):\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}\"\n",
    "    else:\n",
    "        return f\"https://www.streamwithvpn.com/{clean_title}-{int(year)}\"\n",
    "\n",
    "def scrape_movie_data(url, tconst, title, year, endYear, titleType, isAdult, runtime, genres, rating, numVotes):\n",
    "    \"\"\"\n",
    "    Scrape movie/series data from StreamWithVPN\n",
    "    Returns dictionary with description, cast, and streaming platforms\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add delay to be respectful to the server\n",
    "        time.sleep(1)\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code in [404, 403]:\n",
    "                url_without_year = generate_streamwithvpn_url(title, None)\n",
    "                try:\n",
    "                    response = requests.get(url_without_year, headers=headers, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    url = url_without_year\n",
    "                except requests.exceptions.HTTPError:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        movie_data = {\n",
    "            'tconst': tconst,\n",
    "            'titleType': titleType,\n",
    "            'title': title,\n",
    "            'year': year,\n",
    "            'endYear': endYear,\n",
    "            'isAdult': isAdult,\n",
    "            'runtime': runtime,\n",
    "            'genres': genres,\n",
    "            'rating': rating,\n",
    "            'numVotes': numVotes,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'url': url,\n",
    "            'scrape_status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Extract description\n",
    "        description_element = soup.find('span', class_='rt-Text EntryDetailDescription_contentDescription__tXYGO EntryDetailDescription_expanded__3a0Gs')\n",
    "        \n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('EntryDetailDescription_contentDescription'))\n",
    "        if not description_element:\n",
    "            description_element = soup.find('span', class_=re.compile('contentDescription'))\n",
    "        if not description_element:\n",
    "            description_element = soup.select_one('span[class*=\"EntryDetailDescription_contentDescription\"]')\n",
    "        \n",
    "        if description_element:\n",
    "            movie_data['description'] = description_element.get_text(strip=True)\n",
    "            print(f\"✓ Found description for {title}: {movie_data['description'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"✗ No description found for {title}\")\n",
    "        \n",
    "        \"\"\"\n",
    "        # Extract CAST information\n",
    "        cast_list = []\n",
    "        # Target container div\n",
    "        container_div = soup.find('div', class_='rt-Flex rt-r-fd-column rt-r-gap rt-r-px rt-r-pt rt-r-w', style='--gap: 2px; --pl: 16px; --pr: 16px; --pt: 8px; --width: 100%;')\n",
    "        if container_div:\n",
    "            cast_spans = container_div.find_all('span', {'data-accent-color': 'gray', 'class_': 'rt-Text rt-r-size-3 rt-r-weight-medium', 'style': 'min-width: 0px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;'})\n",
    "            if cast_spans:\n",
    "                for span in cast_spans:\n",
    "                    cast_list.append(span.get_text(strip=True))\n",
    "                movie_data['cast'] = ', '.join(cast_list)\n",
    "                print(f\"✓ Found cast for {title}: {movie_data['cast'][:100]}...\")\n",
    "            else:\n",
    "                print(f\"✗ Span not found for {title}\")\n",
    "        else:\n",
    "            print(f\"✗ Div not found for {title}\")\n",
    "\n",
    "        # Extract STREAMING PLATFORMS information\n",
    "        platform_elements = soup.find_all('h2', class_='rt-Heading rt-r-size-5 rt-r-weight-medium rt-r-ta-left')\n",
    "        if platform_elements:\n",
    "            platforms = [elem.get_text(strip=True) for elem in platform_elements]\n",
    "            movie_data['streaming_platforms'] = ', '.join(platforms)\n",
    "        else:\n",
    "            print(f\"✗ No streaming platforms found for {title}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        return movie_data\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'request_error: {str(e)}'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'tconst': tconst,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'description': None,\n",
    "            'cast': None,\n",
    "            'streaming_platforms': None,\n",
    "            'scrape_status': f'parsing_error: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Initialize variables\n",
    "chunk_size = 500  # Process 500 movies at a time\n",
    "total_chunks = len(top_10000_df) // chunk_size + 1\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in chunks\n",
    "for chunk_start in tqdm(range(0, len(top_10000_df), chunk_size), desc=\"Processing chunks\"):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(top_10000_df))\n",
    "    chunk_df = top_10000_df.iloc[chunk_start:chunk_end]\n",
    "    chunk_data = []\n",
    "    \n",
    "    # Process each movie in the chunk\n",
    "    for _, row in tqdm(chunk_df.iterrows(), total=len(chunk_df), desc=f\"Chunk {chunk_start//chunk_size + 1}/{total_chunks}\", leave=False):\n",
    "        try:\n",
    "            tconst = row['tconst']\n",
    "            title = row['primaryTitle']\n",
    "            year = row['startYear']\n",
    "            endYear = row['endYear']\n",
    "            titleType = row['titleType']\n",
    "            isAdult = row['isAdult']\n",
    "            runtime = row['runtimeMinutes']\n",
    "            genres = row['genres']\n",
    "            rating = row['averageRating']\n",
    "            numVotes = row['numVotes']\n",
    "            \n",
    "            url = generate_streamwithvpn_url(title, year)\n",
    "            movie_data = scrape_movie_data(url, tconst, title, year, endYear, \n",
    "                                         titleType, isAdult, runtime, genres, \n",
    "                                         rating, numVotes)\n",
    "            chunk_data.append(movie_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {title}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save chunk progress\n",
    "    chunk_df = pd.DataFrame(chunk_data)\n",
    "    chunk_df.to_csv(f'data/temp_scrape_chunk_{chunk_start}.csv', sep='\\t', index=False)\n",
    "    \n",
    "    # Print progress stats\n",
    "    elapsed = time.time() - start_time\n",
    "    processed = chunk_end\n",
    "    remaining = len(top_10000_df) - processed\n",
    "    rate = processed / elapsed\n",
    "    eta = remaining / rate if rate > 0 else 0\n",
    "    \n",
    "    print(f\"\\nChunk {chunk_start}-{chunk_end} completed\")\n",
    "    print(f\"Processed {processed:,}/{len(top_10000_df):,} movies in {elapsed/3600:.2f} hours\")\n",
    "    print(f\"Estimated time remaining: {eta/3600:.2f} hours\")\n",
    "    print(f\"Success rate in chunk: {len(chunk_df[chunk_df['scrape_status'] == 'success'])} / {len(chunk_df)}\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"\\nCombining chunks...\")\n",
    "all_chunks = []\n",
    "for chunk_start in range(0, len(top_10000_df), chunk_size):\n",
    "    try:\n",
    "        chunk = pd.read_csv(f'data/temp_scrape_chunk_{chunk_start}.csv', sep='\\t')\n",
    "        all_chunks.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading chunk {chunk_start}: {str(e)}\")\n",
    "\n",
    "scraped_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# Save final results\n",
    "scraped_df.to_csv('data/top10000_final.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Print final statistics\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nProcessing completed in {total_time/3600:.2f} hours\")\n",
    "print(f\"Total movies processed: {len(scraped_df):,}\")\n",
    "print(f\"Overall success rate: {len(scraped_df[scraped_df['scrape_status'] == 'success']):,} / {len(scraped_df):,}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "import os\n",
    "for chunk_start in range(0, len(top_10000_df), chunk_size):\n",
    "    try:\n",
    "        os.remove(f'data/temp_scrape_chunk_{chunk_start}.csv')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a456fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2012 failed scraping attempts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying failed scrapes:  20%|██        | 1/5 [31:14<2:04:57, 1874.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry Chunk 0-500 completed\n",
      "Processed 500/2,012 movies in 0.52 hours\n",
      "Estimated time remaining: 1.57 hours\n",
      "Success rate in retry chunk: 0 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying failed scrapes:  40%|████      | 2/5 [1:02:29<1:33:43, 1874.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry Chunk 500-1000 completed\n",
      "Processed 1,000/2,012 movies in 1.04 hours\n",
      "Estimated time remaining: 1.05 hours\n",
      "Success rate in retry chunk: 0 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying failed scrapes:  60%|██████    | 3/5 [1:33:46<1:02:31, 1875.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry Chunk 1000-1500 completed\n",
      "Processed 1,500/2,012 movies in 1.56 hours\n",
      "Estimated time remaining: 0.53 hours\n",
      "Success rate in retry chunk: 0 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying failed scrapes:  80%|████████  | 4/5 [2:05:03<31:16, 1876.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry Chunk 1500-2000 completed\n",
      "Processed 2,000/2,012 movies in 2.08 hours\n",
      "Estimated time remaining: 0.01 hours\n",
      "Success rate in retry chunk: 0 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying failed scrapes: 100%|██████████| 5/5 [2:05:49<00:00, 1509.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry Chunk 2000-2012 completed\n",
      "Processed 2,012/2,012 movies in 2.10 hours\n",
      "Estimated time remaining: 0.00 hours\n",
      "Success rate in retry chunk: 0 / 12\n",
      "\n",
      "Retry processing completed in 2.10 hours\n",
      "Initial failed scrapes: 2,012\n",
      "Successful retries: 0\n",
      "Final success rate: 7,988 / 10,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the previously scraped data\n",
    "scraped_df = pd.read_csv('data/top10000_final.tsv', sep='\\t')\n",
    "\n",
    "# Identify failed scraping attempts\n",
    "failed_scrapes = scraped_df[scraped_df['scrape_status'] != 'success']\n",
    "print(f\"\\nFound {len(failed_scrapes)} failed scraping attempts\")\n",
    "\n",
    "# Initialize variables for retry\n",
    "chunk_size = 500\n",
    "total_chunks = len(failed_scrapes) // chunk_size + 1\n",
    "start_time = time.time()\n",
    "retry_data = []\n",
    "\n",
    "# Process failed scrapes in chunks\n",
    "for chunk_start in tqdm(range(0, len(failed_scrapes), chunk_size), desc=\"Retrying failed scrapes\"):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(failed_scrapes))\n",
    "    chunk_df = failed_scrapes.iloc[chunk_start:chunk_end]\n",
    "    chunk_data = []\n",
    "    \n",
    "    # Process each failed movie in the chunk\n",
    "    for _, row in tqdm(chunk_df.iterrows(), total=len(chunk_df), desc=f\"Retry Chunk {chunk_start//chunk_size + 1}/{total_chunks}\", leave=False):\n",
    "        try:\n",
    "            tconst = row['tconst']\n",
    "            title = row['title']\n",
    "            year = row['year']\n",
    "            endYear = row['endYear']\n",
    "            titleType = row['titleType']\n",
    "            isAdult = row['isAdult']\n",
    "            runtime = row['runtime']\n",
    "            genres = row['genres']\n",
    "            rating = row['rating']\n",
    "            numVotes = row['numVotes']\n",
    "            \n",
    "            # Try scraping again with increased delay\n",
    "            time.sleep(2)  # Increased delay for retry attempts\n",
    "            url = generate_streamwithvpn_url(title, year)\n",
    "            movie_data = scrape_movie_data(url, tconst, title, year, endYear, \n",
    "                                         titleType, isAdult, runtime, genres, \n",
    "                                         rating, numVotes)\n",
    "            chunk_data.append(movie_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {title}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save retry progress\n",
    "    retry_chunk_df = pd.DataFrame(chunk_data)\n",
    "    retry_data.extend(chunk_data)\n",
    "    \n",
    "    # Print progress stats\n",
    "    elapsed = time.time() - start_time\n",
    "    processed = chunk_end\n",
    "    remaining = len(failed_scrapes) - processed\n",
    "    rate = processed / elapsed\n",
    "    eta = remaining / rate if rate > 0 else 0\n",
    "    \n",
    "    print(f\"\\nRetry Chunk {chunk_start}-{chunk_end} completed\")\n",
    "    print(f\"Processed {processed:,}/{len(failed_scrapes):,} movies in {elapsed/3600:.2f} hours\")\n",
    "    print(f\"Estimated time remaining: {eta/3600:.2f} hours\")\n",
    "    print(f\"Success rate in retry chunk: {len(retry_chunk_df[retry_chunk_df['scrape_status'] == 'success'])} / {len(retry_chunk_df)}\")\n",
    "\n",
    "# Convert retry results to DataFrame\n",
    "retry_df = pd.DataFrame(retry_data)\n",
    "\n",
    "# Update original DataFrame with successful retries\n",
    "successful_retries = retry_df[retry_df['scrape_status'] == 'success']\n",
    "for _, retry_row in successful_retries.iterrows():\n",
    "    scraped_df.loc[scraped_df['tconst'] == retry_row['tconst']] = retry_row\n",
    "\n",
    "# Save updated results\n",
    "scraped_df.to_csv('data/top10000_final_with_retries.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Print final statistics\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nRetry processing completed in {total_time/3600:.2f} hours\")\n",
    "print(f\"Initial failed scrapes: {len(failed_scrapes):,}\")\n",
    "print(f\"Successful retries: {len(successful_retries):,}\")\n",
    "print(f\"Final success rate: {len(scraped_df[scraped_df['scrape_status'] == 'success']):,} / {len(scraped_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00ec40",
   "metadata": {},
   "source": [
    "## Unify Description and Cast data in 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fec8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "top10000_cast = pd.read_csv('data/cast_data/top_10000_with_cast.tsv', sep='\\t')\n",
    "top10000_description = pd.read_csv('data/top10000_final.tsv', sep='\\t')\n",
    "\n",
    "# Merge cast and reviews data on the 'tconst' column by getting only the top10000_cast columns and adding the description from top10000_description\n",
    "merged_df = top10000_cast.merge(top10000_description[['tconst', 'description']], on='tconst', how='inner')\n",
    "# Save the merged dataframe to a new TSV file\n",
    "merged_df.to_csv('data/top_10000_with_cast_and_description.tsv', sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a629252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser error: Error tokenizing data. C error: Expected 1 fields in line 5967, saw 2\n",
      "\n",
      "Reviews loaded with some lines skipped due to parsing errors\n",
      "Total null descriptions: 2024\n",
      "\n",
      "Null descriptions by 500-entry chunks:\n",
      "----------------------------------------\n",
      "Entries    1- 500:  54 nulls out of 500 ( 10.8%)\n",
      "Entries  501-1000:  52 nulls out of 500 ( 10.4%)\n",
      "Entries 1001-1500:  62 nulls out of 500 ( 12.4%)\n",
      "Entries 1501-2000:  75 nulls out of 500 ( 15.0%)\n",
      "Entries 2001-2500:  51 nulls out of 500 ( 10.2%)\n",
      "Entries 2501-3000:  88 nulls out of 500 ( 17.6%)\n",
      "Entries 3001-3500:  82 nulls out of 500 ( 16.4%)\n",
      "Entries 3501-4000:  90 nulls out of 500 ( 18.0%)\n",
      "Entries 4001-4500:  92 nulls out of 500 ( 18.4%)\n",
      "Entries 4501-5000:  94 nulls out of 500 ( 18.8%)\n",
      "Entries 5001-5500: 108 nulls out of 500 ( 21.6%)\n",
      "Entries 5501-6000: 111 nulls out of 500 ( 22.2%)\n",
      "Entries 6001-6500: 113 nulls out of 500 ( 22.6%)\n",
      "Entries 6501-7000: 122 nulls out of 500 ( 24.4%)\n",
      "Entries 7001-7500: 122 nulls out of 500 ( 24.4%)\n",
      "Entries 7501-8000: 117 nulls out of 500 ( 23.4%)\n",
      "Entries 8001-8500: 137 nulls out of 500 ( 27.4%)\n",
      "Entries 8501-9000: 155 nulls out of 500 ( 31.0%)\n",
      "Entries 9001-9500: 149 nulls out of 500 ( 29.8%)\n",
      "Entries 9501-10000: 150 nulls out of 500 ( 30.0%)\n",
      "----------------------------------------\n",
      "Overall: 2024 nulls out of 10000 (20.2%)\n",
      "\n",
      "After removing null descriptions, new shape: (7976, 13)\n",
      "Reviews loaded with some lines skipped due to parsing errors\n",
      "Total null descriptions: 2024\n",
      "\n",
      "Null descriptions by 500-entry chunks:\n",
      "----------------------------------------\n",
      "Entries    1- 500:  54 nulls out of 500 ( 10.8%)\n",
      "Entries  501-1000:  52 nulls out of 500 ( 10.4%)\n",
      "Entries 1001-1500:  62 nulls out of 500 ( 12.4%)\n",
      "Entries 1501-2000:  75 nulls out of 500 ( 15.0%)\n",
      "Entries 2001-2500:  51 nulls out of 500 ( 10.2%)\n",
      "Entries 2501-3000:  88 nulls out of 500 ( 17.6%)\n",
      "Entries 3001-3500:  82 nulls out of 500 ( 16.4%)\n",
      "Entries 3501-4000:  90 nulls out of 500 ( 18.0%)\n",
      "Entries 4001-4500:  92 nulls out of 500 ( 18.4%)\n",
      "Entries 4501-5000:  94 nulls out of 500 ( 18.8%)\n",
      "Entries 5001-5500: 108 nulls out of 500 ( 21.6%)\n",
      "Entries 5501-6000: 111 nulls out of 500 ( 22.2%)\n",
      "Entries 6001-6500: 113 nulls out of 500 ( 22.6%)\n",
      "Entries 6501-7000: 122 nulls out of 500 ( 24.4%)\n",
      "Entries 7001-7500: 122 nulls out of 500 ( 24.4%)\n",
      "Entries 7501-8000: 117 nulls out of 500 ( 23.4%)\n",
      "Entries 8001-8500: 137 nulls out of 500 ( 27.4%)\n",
      "Entries 8501-9000: 155 nulls out of 500 ( 31.0%)\n",
      "Entries 9001-9500: 149 nulls out of 500 ( 29.8%)\n",
      "Entries 9501-10000: 150 nulls out of 500 ( 30.0%)\n",
      "----------------------------------------\n",
      "Overall: 2024 nulls out of 10000 (20.2%)\n",
      "\n",
      "After removing null descriptions, new shape: (7976, 13)\n"
     ]
    }
   ],
   "source": [
    "top10000 = pd.read_csv('data/top_10000_with_cast_and_description.tsv', sep='\\t')\n",
    "\n",
    "# Try to read the reviews file with error handling\n",
    "try:\n",
    "    reviews = pd.read_csv('data/reviews_data/filtered_reviews2.csv', sep='\\t')\n",
    "    print(\"Reviews loaded successfully\")\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Parser error: {e}\")\n",
    "    # Try with different options to handle malformed data\n",
    "    try:\n",
    "        reviews = pd.read_csv('data/reviews_data/filtered_reviews2.csv', \n",
    "                            sep='\\t', \n",
    "                            on_bad_lines='skip',  # Skip bad lines\n",
    "                            engine='python')     # Use Python engine for better error handling\n",
    "        print(\"Reviews loaded with some lines skipped due to parsing errors\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Still failed: {e2}\")\n",
    "        # Try with comma separator instead\n",
    "        try:\n",
    "            reviews = pd.read_csv('data/reviews_data/filtered_reviews2.csv', \n",
    "                                sep=',',  # Try comma separator\n",
    "                                on_bad_lines='skip')\n",
    "            print(\"Reviews loaded using comma separator\")\n",
    "        except Exception as e3:\n",
    "            print(f\"All attempts failed: {e3}\")\n",
    "            reviews = None\n",
    "\n",
    "# Get different values of isAdult\n",
    "#print(top10000['isAdult'].value_counts())\n",
    "# Remove isAdult column\n",
    "#top10000 = top10000.drop(columns=['isAdult'])\n",
    "#top10000.to_csv('data/top_10000_with_cast_and_description.tsv', sep='\\t', index=False)\n",
    "\n",
    "#print((top10000[top10000['primaryTitle'] != top10000['originalTitle']]).shape[0])\n",
    "\n",
    "# Total null descriptions\n",
    "total_nulls = top10000[top10000['description'].isna()].shape[0]\n",
    "print(f\"Total null descriptions: {total_nulls}\")\n",
    "\n",
    "# Analyze null descriptions in chunks of 500\n",
    "chunk_size = 500\n",
    "total_entries = len(top10000)\n",
    "\n",
    "print(\"\\nNull descriptions by 500-entry chunks:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for chunk_start in range(0, total_entries, chunk_size):\n",
    "    chunk_end = min(chunk_start + chunk_size, total_entries)\n",
    "    chunk_data = top10000.iloc[chunk_start:chunk_end]\n",
    "    \n",
    "    null_count = chunk_data[chunk_data['description'].isna()].shape[0]\n",
    "    chunk_total = len(chunk_data)\n",
    "    percentage = (null_count / chunk_total) * 100 if chunk_total > 0 else 0\n",
    "    \n",
    "    print(f\"Entries {chunk_start+1:4d}-{chunk_end:4d}: {null_count:3d} nulls out of {chunk_total:3d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Overall: {total_nulls} nulls out of {total_entries} ({(total_nulls/total_entries)*100:.1f}%)\")\n",
    "\n",
    "'''\n",
    "# Only show reviews if they loaded successfully\n",
    "if reviews is not None:\n",
    "    print(f\"\\nReviews data shape: {reviews.shape}\")\n",
    "    print(reviews[reviews['tconst']=='tt0903747'])\n",
    "else:\n",
    "    print(\"\\nCould not load reviews data\")\n",
    "\n",
    "# Remove lines with no description\n",
    "top10000_cleaned = top10000.dropna(subset=['description'])\n",
    "print(f\"\\nAfter removing null descriptions, new shape: {top10000_cleaned.shape}\")\n",
    "top10000_cleaned.to_csv('data/FinalData.tsv', sep='\\t', index=False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvACFinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
